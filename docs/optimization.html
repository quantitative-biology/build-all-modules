<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>8 Optimization | Quant Bio Modules</title>
  <meta name="description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="generator" content="bookdown 0.22 and GitBook 2.6.7" />

  <meta property="og:title" content="8 Optimization | Quant Bio Modules" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A set of modules on quantitaitve topics for biology graduates" />
  <meta name="github-repo" content="rstudio/bookdown-demo" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="8 Optimization | Quant Bio Modules" />
  
  <meta name="twitter:description" content="A set of modules on quantitaitve topics for biology graduates" />
  

<meta name="author" content="Kim Cuddington, Andrew Edwards, Brian Ingalls" />


<meta name="date" content="2021-11-02" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="machine-learning.html"/>

<script src="libs/jquery-3.5.1/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />
<link href="libs/bsTable-3.3.7/bootstrapTable.min.css" rel="stylesheet" />
<script src="libs/bsTable-3.3.7/bootstrapTable.js"></script>
<script src="libs/htmlwidgets-1.5.3/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.9.4/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.1.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.1/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-1.57.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-1.57.1/plotly-latest.min.js"></script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> Prerequisites</a></li>
<li class="chapter" data-level="2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html"><i class="fa fa-check"></i><b>2</b> Introduction to Git and GitHub</a><ul>
<li class="chapter" data-level="2.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-are-git-and-github-and-how-might-they-be-useful"><i class="fa fa-check"></i><b>2.1</b> What are Git and GitHub and how might they be useful?</a></li>
<li class="chapter" data-level="2.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#you-just-want-do-download-someone-elses-code-from-their-github-repository"><i class="fa fa-check"></i><b>2.2</b> You just want do download someone else’s code from their GitHub repository</a></li>
<li class="chapter" data-level="2.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#motivation-for-learning-more"><i class="fa fa-check"></i><b>2.3</b> Motivation for learning more</a><ul>
<li class="chapter" data-level="2.3.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#example-application-pacific-hake-stock-assessment"><i class="fa fa-check"></i><b>2.3.1</b> Example application – Pacific Hake stock assessment</a></li>
<li class="chapter" data-level="2.3.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#examples-of-what-we-can-avoid"><i class="fa fa-check"></i><b>2.3.2</b> Examples of what we can avoid</a></li>
<li class="chapter" data-level="2.3.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#example-of-advantages-that-arise-from-using-github"><i class="fa fa-check"></i><b>2.3.3</b> Example of advantages that arise from using GitHub</a></li>
<li class="chapter" data-level="2.3.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#why-this-course"><i class="fa fa-check"></i><b>2.3.4</b> Why this course?</a></li>
<li class="chapter" data-level="2.3.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#does-it-matter-which-computer-language-my-code-is-in"><i class="fa fa-check"></i><b>2.3.5</b> Does it matter which computer language my code is in?</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#getting-set-up-for-the-first-time"><i class="fa fa-check"></i><b>2.4</b> Getting set up for the first time</a><ul>
<li class="chapter" data-level="2.4.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-you-will-end-up-having-installed"><i class="fa fa-check"></i><b>2.4.1</b> What you will end up having installed</a></li>
<li class="chapter" data-level="2.4.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#text-editor"><i class="fa fa-check"></i><b>2.4.2</b> Text Editor</a></li>
<li class="chapter" data-level="2.4.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-the-git-application-on-your-machine"><i class="fa fa-check"></i><b>2.4.3</b> Install the Git application on your machine</a></li>
<li class="chapter" data-level="2.4.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-shell"><i class="fa fa-check"></i><b>2.4.4</b> Git shell</a></li>
<li class="chapter" data-level="2.4.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-shell-rstudio"><i class="fa fa-check"></i><b>2.4.5</b> Git shell, RStudio</a></li>
<li class="chapter" data-level="2.4.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#powershell-and-posh-git"><i class="fa fa-check"></i><b>2.4.6</b> Powershell and posh-git</a></li>
<li class="chapter" data-level="2.4.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#one-time-authentication"><i class="fa fa-check"></i><b>2.4.7</b> One-time authentication</a></li>
<li class="chapter" data-level="2.4.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#configure-the-git-application"><i class="fa fa-check"></i><b>2.4.8</b> Configure the Git application</a></li>
<li class="chapter" data-level="2.4.9" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#install-a-difftool-optional"><i class="fa fa-check"></i><b>2.4.9</b> Install a difftool (optional)</a></li>
<li class="chapter" data-level="2.4.10" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#save-our-template-.gitconfig-file"><i class="fa fa-check"></i><b>2.4.10</b> Save our template <em>.gitconfig</em> file</a></li>
<li class="chapter" data-level="2.4.11" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#edit-the-.gitconfig-file"><i class="fa fa-check"></i><b>2.4.11</b> Edit the <em>.gitconfig</em> file</a></li>
<li class="chapter" data-level="2.4.12" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#mac-only-make-your-output-pretty"><i class="fa fa-check"></i><b>2.4.12</b> MAC only: make your output pretty</a></li>
<li class="chapter" data-level="2.4.13" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#navigating-in-a-shell"><i class="fa fa-check"></i><b>2.4.13</b> Navigating in a shell</a></li>
<li class="chapter" data-level="2.4.14" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#something-to-view-markdown-files-optional"><i class="fa fa-check"></i><b>2.4.14</b> Something to view Markdown files (optional)</a></li>
<li class="chapter" data-level="2.4.15" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#bonus-keyboard-shortcut"><i class="fa fa-check"></i><b>2.4.15</b> Bonus keyboard shortcut</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-git-and-github-to-share-your-own-code"><i class="fa fa-check"></i><b>2.5</b> Using Git and GitHub to share your own code</a><ul>
<li class="chapter" data-level="2.5.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#definitions"><i class="fa fa-check"></i><b>2.5.1</b> Definitions</a></li>
<li class="chapter" data-level="2.5.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#creating-a-new-repository"><i class="fa fa-check"></i><b>2.5.2</b> Creating a new repository</a></li>
<li class="chapter" data-level="2.5.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#cloning-your-new-repository"><i class="fa fa-check"></i><b>2.5.3</b> Cloning your new repository</a></li>
<li class="chapter" data-level="2.5.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#committing"><i class="fa fa-check"></i><b>2.5.4</b> Committing</a></li>
<li class="chapter" data-level="2.5.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-1-create-edit-and-commit-simpletext.txt"><i class="fa fa-check"></i><b>2.5.5</b> Exercise 1: create, edit and commit <em>simpleText.txt</em></a></li>
<li class="chapter" data-level="2.5.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-2-multiple-files"><i class="fa fa-check"></i><b>2.5.6</b> Exercise 2: multiple files</a></li>
<li class="chapter" data-level="2.5.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#what-to-write-in-commit-messages"><i class="fa fa-check"></i><b>2.5.7</b> What to write in commit messages</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-git-and-github-to-collaborate-with-colleagues"><i class="fa fa-check"></i><b>2.6</b> Using Git and GitHub to collaborate with colleagues</a><ul>
<li class="chapter" data-level="2.6.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#demonstration-of-collaborating"><i class="fa fa-check"></i><b>2.6.1</b> Demonstration of collaborating</a></li>
<li class="chapter" data-level="2.6.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#a-bit-more-about-git-rebase"><i class="fa fa-check"></i><b>2.6.2</b> A bit more about git rebase</a></li>
<li class="chapter" data-level="2.6.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#fixing-a-conflict"><i class="fa fa-check"></i><b>2.6.3</b> Fixing a conflict</a></li>
<li class="chapter" data-level="2.6.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#exercise-3-collaborating-on-a-single-repository"><i class="fa fa-check"></i><b>2.6.4</b> Exercise 3: collaborating on a single repository</a></li>
<li class="chapter" data-level="2.6.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#collaborating-summary"><i class="fa fa-check"></i><b>2.6.5</b> Collaborating summary</a></li>
</ul></li>
<li class="chapter" data-level="2.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#workflow-tips-when-collaborating"><i class="fa fa-check"></i><b>2.7</b> Workflow tips when collaborating</a></li>
<li class="chapter" data-level="2.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#beyond-the-basics-of-git-and-github-getting-more-advanced"><i class="fa fa-check"></i><b>2.8</b> Beyond the basics of Git and GitHub – getting more advanced</a><ul>
<li class="chapter" data-level="2.8.1" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-ive-made-some-changes-but-dont-really-want-to-keep-them-git-stash"><i class="fa fa-check"></i><b>2.8.1</b> So I’ve made some changes but don’t really want to keep them – git stash</a></li>
<li class="chapter" data-level="2.8.2" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#the-power-to-go-back"><i class="fa fa-check"></i><b>2.8.2</b> The power to go back</a></li>
<li class="chapter" data-level="2.8.3" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#so-how-does-git-do-all-this"><i class="fa fa-check"></i><b>2.8.3</b> So how does Git do all this?</a></li>
<li class="chapter" data-level="2.8.4" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#git-terminology"><i class="fa fa-check"></i><b>2.8.4</b> Git terminology</a></li>
<li class="chapter" data-level="2.8.5" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#branching"><i class="fa fa-check"></i><b>2.8.5</b> Branching</a></li>
<li class="chapter" data-level="2.8.6" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#pull-requests"><i class="fa fa-check"></i><b>2.8.6</b> Pull requests</a></li>
<li class="chapter" data-level="2.8.7" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#undoing-stuff"><i class="fa fa-check"></i><b>2.8.7</b> Undoing stuff</a></li>
<li class="chapter" data-level="2.8.8" data-path="introduction-to-git-and-github.html"><a href="introduction-to-git-and-github.html#using-r-and-github-within-rstudio"><i class="fa fa-check"></i><b>2.8.8</b> Using R and GitHub within RStudio</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html"><i class="fa fa-check"></i><b>3</b> Introduction to R Markdown</a><ul>
<li class="chapter" data-level="3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#motivation"><i class="fa fa-check"></i><b>3.1</b> Motivation</a></li>
<li class="chapter" data-level="3.2" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#basic-idea"><i class="fa fa-check"></i><b>3.2</b> Basic idea</a></li>
<li class="chapter" data-level="3.3" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#simple-example"><i class="fa fa-check"></i><b>3.3</b> Simple example</a><ul>
<li class="chapter" data-level="3.3.1" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#exercise"><i class="fa fa-check"></i><b>3.3.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#output-format"><i class="fa fa-check"></i><b>3.4</b> Output format</a></li>
<li class="chapter" data-level="3.5" data-path="introduction-to-r-markdown.html"><a href="introduction-to-r-markdown.html#further-reading"><i class="fa fa-check"></i><b>3.5</b> Further reading</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="introduction-to-multivariate-analysis.html"><a href="introduction-to-multivariate-analysis.html"><i class="fa fa-check"></i><b>4</b> Introduction to multivariate analysis</a></li>
<li class="chapter" data-level="5" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html"><i class="fa fa-check"></i><b>5</b> Multivariate distance and cluster analysis</a><ul>
<li class="chapter" data-level="5.1" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#thinking-about-resemblence"><i class="fa fa-check"></i><b>5.1</b> Thinking about resemblence</a></li>
<li class="chapter" data-level="5.2" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#binary-similarity-metrics"><i class="fa fa-check"></i><b>5.2</b> Binary Similarity metrics</a></li>
<li class="chapter" data-level="5.3" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#quantitative-similarity-dissimilarity-metrics"><i class="fa fa-check"></i><b>5.3</b> Quantitative similarity &amp; dissimilarity metrics</a><ul>
<li class="chapter" data-level="5.3.1" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#euclidean-distance"><i class="fa fa-check"></i><b>5.3.1</b> Euclidean Distance</a></li>
<li class="chapter" data-level="5.3.2" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#comparing-more-than-two-communitiessamplessitesgenesspecies"><i class="fa fa-check"></i><b>5.3.2</b> Comparing more than two communities/samples/sites/genes/species</a></li>
<li class="chapter" data-level="5.3.3" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#r-functions"><i class="fa fa-check"></i><b>5.3.3</b> R functions</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#cluster-analysis"><i class="fa fa-check"></i><b>5.4</b> Cluster Analysis</a><ul>
<li class="chapter" data-level="5.4.1" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#hierarchical-clustering-groups-are-nested-within-other-groups."><i class="fa fa-check"></i><b>5.4.1</b> Hierarchical clustering: groups are nested within other groups.</a></li>
<li class="chapter" data-level="5.4.2" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#r-functions-1"><i class="fa fa-check"></i><b>5.4.2</b> R functions</a></li>
<li class="chapter" data-level="5.4.3" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#how-many-clusters"><i class="fa fa-check"></i><b>5.4.3</b> How many clusters</a></li>
<li class="chapter" data-level="5.4.4" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#k-means-clustering"><i class="fa fa-check"></i><b>5.4.4</b> K-Means Clustering</a></li>
<li class="chapter" data-level="5.4.5" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#fuzzy-c-means-clustering"><i class="fa fa-check"></i><b>5.4.5</b> Fuzzy C-Means Clustering</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="multivariate-distance-and-cluster-analysis.html"><a href="multivariate-distance-and-cluster-analysis.html#exercise-cluster-analysis-of-isotope-data"><i class="fa fa-check"></i><b>5.5</b> Exercise: Cluster analysis of isotope data</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ordination.html"><a href="ordination.html"><i class="fa fa-check"></i><b>6</b> Ordination</a><ul>
<li class="chapter" data-level="6.1" data-path="ordination.html"><a href="ordination.html#ordination-as-data-reduction"><i class="fa fa-check"></i><b>6.1</b> Ordination as data reduction</a></li>
<li class="chapter" data-level="6.2" data-path="ordination.html"><a href="ordination.html#methods-of-ordination"><i class="fa fa-check"></i><b>6.2</b> Methods of Ordination</a></li>
<li class="chapter" data-level="6.3" data-path="ordination.html"><a href="ordination.html#principal-components-analysis-pca"><i class="fa fa-check"></i><b>6.3</b> Principal Components Analysis (PCA)</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="machine-learning.html"><a href="machine-learning.html"><i class="fa fa-check"></i><b>7</b> Machine learning</a><ul>
<li class="chapter" data-level="7.1" data-path="machine-learning.html"><a href="machine-learning.html#classification"><i class="fa fa-check"></i><b>7.1</b> Classification</a><ul>
<li class="chapter" data-level="7.1.1" data-path="machine-learning.html"><a href="machine-learning.html#logistic-regression-as-a-binary-classifier"><i class="fa fa-check"></i><b>7.1.1</b> Logistic regression as a binary classifier</a></li>
<li class="chapter" data-level="7.1.2" data-path="machine-learning.html"><a href="machine-learning.html#interpreting-the-logistic-regression"><i class="fa fa-check"></i><b>7.1.2</b> Interpreting the logistic regression</a></li>
<li class="chapter" data-level="7.1.3" data-path="machine-learning.html"><a href="machine-learning.html#measuring-the-performance-of-a-binary-classifier"><i class="fa fa-check"></i><b>7.1.3</b> Measuring the performance of a binary classifier</a></li>
<li class="chapter" data-level="7.1.4" data-path="machine-learning.html"><a href="machine-learning.html#multiple-logistic-regression"><i class="fa fa-check"></i><b>7.1.4</b> Multiple logistic regression</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="machine-learning.html"><a href="machine-learning.html#cross-validation"><i class="fa fa-check"></i><b>7.2</b> Cross-validation</a><ul>
<li class="chapter" data-level="7.2.1" data-path="machine-learning.html"><a href="machine-learning.html#k-fold-cross-validation"><i class="fa fa-check"></i><b>7.2.1</b> k-Fold Cross-Validation</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="machine-learning.html"><a href="machine-learning.html#tree-based-methods-for-classification"><i class="fa fa-check"></i><b>7.3</b> Tree-based methods for classification</a><ul>
<li class="chapter" data-level="7.3.1" data-path="machine-learning.html"><a href="machine-learning.html#classification-and-regression-trees-carts"><i class="fa fa-check"></i><b>7.3.1</b> Classification and regression trees (CARTs)</a></li>
<li class="chapter" data-level="7.3.2" data-path="machine-learning.html"><a href="machine-learning.html#tree-pruning"><i class="fa fa-check"></i><b>7.3.2</b> Tree pruning</a></li>
<li class="chapter" data-level="7.3.3" data-path="machine-learning.html"><a href="machine-learning.html#random-forests"><i class="fa fa-check"></i><b>7.3.3</b> Random Forests</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="optimization.html"><a href="optimization.html"><i class="fa fa-check"></i><b>8</b> Optimization</a><ul>
<li class="chapter" data-level="8.1" data-path="optimization.html"><a href="optimization.html#introduction"><i class="fa fa-check"></i><b>8.1</b> Introduction</a></li>
<li class="chapter" data-level="8.2" data-path="optimization.html"><a href="optimization.html#fundamentals-of-optimization"><i class="fa fa-check"></i><b>8.2</b> Fundamentals of Optimization</a><ul>
<li class="chapter" data-level="8.2.1" data-path="optimization.html"><a href="optimization.html#fermats-theorem"><i class="fa fa-check"></i><b>8.2.1</b> Fermat’s Theorem</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="optimization.html"><a href="optimization.html#regression"><i class="fa fa-check"></i><b>8.3</b> Regression</a><ul>
<li class="chapter" data-level="8.3.1" data-path="optimization.html"><a href="optimization.html#linear-regression"><i class="fa fa-check"></i><b>8.3.1</b> Linear Regression</a></li>
<li class="chapter" data-level="8.3.2" data-path="optimization.html"><a href="optimization.html#nonlinear-regression"><i class="fa fa-check"></i><b>8.3.2</b> Nonlinear Regression</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="optimization.html"><a href="optimization.html#iterative-optimization-algorithms"><i class="fa fa-check"></i><b>8.4</b> Iterative Optimization Algorithms</a><ul>
<li class="chapter" data-level="8.4.1" data-path="optimization.html"><a href="optimization.html#gradient-descent"><i class="fa fa-check"></i><b>8.4.1</b> Gradient Descent</a></li>
<li class="chapter" data-level="8.4.2" data-path="optimization.html"><a href="optimization.html#global-optimization"><i class="fa fa-check"></i><b>8.4.2</b> Global Optimization</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="optimization.html"><a href="optimization.html#calibration-of-dynamic-models"><i class="fa fa-check"></i><b>8.5</b> Calibration of Dynamic Models</a></li>
<li class="chapter" data-level="8.6" data-path="optimization.html"><a href="optimization.html#uncertainty-analysis-and-bayesian-calibration"><i class="fa fa-check"></i><b>8.6</b> Uncertainty Analysis and Bayesian Calibration</a></li>
<li class="chapter" data-level="8.7" data-path="optimization.html"><a href="optimization.html#references"><i class="fa fa-check"></i><b>8.7</b> References</a></li>
</ul></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Quant Bio Modules</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="optimization" class="section level1">
<h1><span class="header-section-number">8</span> Optimization</h1>
<div id="introduction" class="section level2">
<h2><span class="header-section-number">8.1</span> Introduction</h2>
<p>
To improve is to make better; to optimize is to make best. Optimization is the act of identifying the extreme (cheapest, tallest, fastest…) over a collection of possibilities. Optimization over <strong>design space</strong> (also called <strong>decision space</strong>) is a critical feature of many engineering tasks, and has a role in most areas of applied science, including biology. Examples include optimal manipulation of biological systems (e.g. optimal harvesting or optimal drug dosing) or optimal design of biological systems (e.g. robust synthetic genetic circuits). A complementary task is optimal experimental design, which aims to identify the ‘best’ experiment from a collection of possibilities. Model calibration, to be discussed further below, provides another example; here we seek the ‘best fit’ version of a model from a collection of possible options.
</p>
<p>
Optimization is used to investigate natural systems (i.e. in ‘pure’ science) in cases where nature presents us with an optimal version of a given phenomenon. For example, the principle of energy minimization justifies a wide variety of phenomena, from the shapes of soap bubbles to the configuration of proteins. Darwinian evolution provides another example. Assuming evolution has arrived at optimal designs, we can apply optimization to understand a variety of biological phenomena, from metabolic flux distribution, to brain wiring, to foraging strategies.
</p>
<p>
This module introduces optimization methods in R. Although simple optimization tasks, such as those addressed by introductory calculus, can be accomplished with paper-and-pencil calculations, most optimization tasks of interest in biology demand computational techniques.
</p>
</div>
<div id="fundamentals-of-optimization" class="section level2">
<h2><span class="header-section-number">8.2</span> Fundamentals of Optimization</h2>
<p>Figure 1 illustrates some basics terminology associated with optimization. The graph of a function <span class="math inline">\(f\)</span> of a single variable <span class="math inline">\(x\)</span> is shown, defined over a domain <span class="math inline">\([a,b]\)</span>. In the context of optimization, we can think of each <span class="math inline">\(x\)</span>-value in the interval <span class="math inline">\([a,b]\)</span> as one possible scenario (e.g. enzyme activity, foraging rate, etc.). The function <span class="math inline">\(f\)</span> maps those scenarios to some <strong>objective</strong> (e.g. metabolic flux, fitness) to be optimized (either maximized or minimized). The <em>global</em> extrema (i.e. maximum and minimum) represent the goals of optimization.</p>
<div class="figure"><span id="fig:unnamed-chunk-57"></span>
<img src="../module-5-optimization/Figure1.png" alt="A graph showing local maxima and minima" width="274" height="50%" />
<p class="caption">
Figure 8.1: Figure 1: Extreme Values
</p>
</div>
<p>
More generally, we define <em>local</em> extrema (maxima and minima) as cases that appear to be optimal if we restrict attention only to ‘nearby’ possibilities (i.e. <span class="math inline">\(x\)</span>-values). Most of the theory of optimization methods is dedicated to identifying local optima. These approaches cannot directly identify a global optimum – instead they identify <em>candidate</em> global optima, which then can be compared to one another to identify the ‘best’ result. (A convenient special case occurs for problems where every local optimum is a global optimum; these are called convex optimization problems. Unfortunately, they typically occur only as special cases when addressing biological phenomena.)
</p>
<div id="fermats-theorem" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Fermat’s Theorem</h3>
<p>
To illustrate these fundamentals, consider the following two academic examples that rely on basic calculus, specifically on <strong>Fermat’s Theorem</strong>, which states that local extrema occur at points where the tangent line to a function’s graph is horizontal, i.e. at points where the derivative (the slope of the tangent) is zero (Figure 2).
</p>
<div class="figure"><span id="fig:unnamed-chunk-58"></span>
<img src="../module-5-optimization/Figure2.png" alt="A graph showing local maxima and minima at which horizontal tangents occur" width="236" height="50%" />
<p class="caption">
Figure 8.2: Figure 2: Fermat’s Theorem: local extrema occur at points where the tangent line is horizontal or at endpoints of the domain. Some local extrema are also global extrema.
</p>
</div>
<div id="example-1" class="section level4">
<h4><span class="header-section-number">8.2.1.1</span> Example 1</h4>
<p>
Task: Identify the value of <span class="math inline">\(x\)</span> for which the function <span class="math inline">\(f(x) = x^2+3x-2\)</span> is minimized.
</p>
<p>
Solution: Taking the derivative, we find <span class="math inline">\(f&#39;(x) = 2x+3\)</span>. To determine the point(s) where the derivative is zero, we solve:
<span class="math display">\[f&#39;(x) = 0 \ \ \Leftrightarrow \ \ 2x+3 = 0 \ \ \Leftrightarrow \ \ x = -\frac{3}{2} = -1.5\]</span>
As shown in Figure 3, the single local minimum is the global miminum in this case, so <span class="math inline">\(x=-1.5\)</span> is the desired solution.
</p>
<div class="figure"><span id="fig:unnamed-chunk-59"></span>
<img src="../module-5-optimization/Figure3.png" alt="Graph of $f(x) = x^2+3x-2$" width="245" height="50%" />
<p class="caption">
Figure 8.3: Figure 3: Graph of <span class="math inline">\(f(x) = x^2+3x-2\)</span> (Example 1)
</p>
</div>
</div>
<div id="example-2" class="section level4">
<h4><span class="header-section-number">8.2.1.2</span> Example 2</h4>
<p>
Task: Identify the value of <span class="math inline">\(x\)</span> for which the function <span class="math inline">\(f(x) = 3x^4-4x^3-54x^2+108x\)</span> is minimized.
</p>
<p>
<p>Solution: Taking the derivative, we find <span class="math inline">\(f&#39;(x) = 12x^3-12x^2-108x+108 = 12(x-3)(x-1)(x+3)\)</span>. To determine the point(s) where the derivative is zero, we solve:</p>
<span class="math display">\[f&#39;(x) = 0 \ \ \Leftrightarrow \ \ x = 3, 1, -3\]</span>
As shown in Figure 4, two of these points are local minima and one is a local maximum. Of the two local minima, <span class="math inline">\(x=-3\)</span> is where the global minimum occurs.
</p>
<div class="figure"><span id="fig:unnamed-chunk-60"></span>
<img src="../module-5-optimization/Figure4.png" alt="Graph of $f(x) = 3x^4-4x^3-54x^2+108x$" width="291" height="50%" />
<p class="caption">
Figure 8.4: Figure 4: Graph of <span class="math inline">\(f(x) = 3x^4-4x^3-54x^2+108x\)</span> (Example 2)
</p>
</div>
</div>
</div>
</div>
<div id="regression" class="section level2">
<h2><span class="header-section-number">8.3</span> Regression</h2>
<div id="linear-regression" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Linear Regression</h3>
<p>
As mentioned above, finding the ‘best fit’ from a family of models is a common optimization task in science. The simplest example is <strong>linear regression</strong>: the task of determining a line of best fit through a given set of points.
</p>
<p>
To illustrate, consider the dataset of <span class="math inline">\((x,y)\)</span> pairs shown in Figure 5 below, which we can label as <span class="math inline">\((x_1, y_1)\)</span>, <span class="math inline">\((x_2, y_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\((x_N, y_N)\)</span>. Several lines are displayed. The optimization task is to identify the ‘best’ line: the one that best captures the trend in the data.
</p>
<div class="figure"><span id="fig:unnamed-chunk-61"></span>
<img src="../module-5-optimization/Figure5.png" alt="Points on a plane and potential best fits lines" width="224" height="50%" />
<p class="caption">
Figure 8.5: Figure 5: Finding a line of best fit
</p>
</div>
<p>
To specify this task mathematically, we need to decide on a measure of quality of fit. We start by recognizing that the line will (typically) fail to pass through most of the points in the dataset. Thus, at each point <span class="math inline">\(x_i\)</span> we can define an <strong>error</strong>, which is the difference between the observed <span class="math inline">\(y\)</span>-value and the model <strong>prediction</strong>, i.e. the <span class="math inline">\(y\)</span>-value on the line. If we specify the line as <span class="math inline">\(y=mx+b\)</span>, then the error at <span class="math inline">\(x_i\)</span> can be defined as <span class="math inline">\(e_i = y_i-(mx_i+b)\)</span>. We then need to combine these errors into a single quality-of-fit measure. This is typically done by squaring the errors and adding them together. (Squaring ensures that both under- and over-estimations contribute equally.) We define the <strong>sum of squared errors</strong> (SSE) as :
<span class="math display">\[\mbox{SSE:} \ \ \ e_1^2+e_2^2+ \cdots e_N^2 \ \ = \ \ (y_1-(mx_1+b))^2 + (y_2-(mx_2+b))^2 + \cdots +(y_N-(mx_N+b))^2. \]</span>
</p>
<p>
We can now pose the model-fitting task as an optimization problem. For each line (that is, each assignment of numerical values to <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span>), we associate a corresponding SSE. We seek the values of <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> for which the SSE is a global minimum.
</p>
<div id="example-3" class="section level4">
<h4><span class="header-section-number">8.3.1.1</span> Example 3</h4>
<p>
Consider a simplified version of linear regression, in which we know that our model (line) should pass through the origin (0,0). That is, instead of lines <span class="math inline">\(y=mx+b\)</span>, we will consider only lines of the form <span class="math inline">\(y=mx\)</span>. We thus have a single parameter value to determine: the slope <span class="math inline">\(m\)</span>. To keep the algebra simple we’ll take a tiny dataset consisting of just two points: <span class="math inline">\((x_1, y_1) = (2,3)\)</span> and <span class="math inline">\((x_2, y_2)= (5,4)\)</span>, as indicated in Figure 6, below. The line passes through points <span class="math inline">\((2,2m)\)</span> and <span class="math inline">\((5, 5m)\)</span>.
</p>
<div class="figure"><span id="fig:unnamed-chunk-62"></span>
<img src="../module-5-optimization/Figure6.png" alt="Defining error between points and line." width="254" height="50%" />
<p class="caption">
Figure 8.6: Figure 6: Finding a line of best fit through the origin (Example 3)
</p>
</div>
In this simple case, the sum of squared errors is:
<span class="math display">\[\begin{equation*}
    \mbox{SSE} = \mbox{SSE}(m)= e_1^2+e_2^2 = (3-2m)^2+(4-5m)^2
\end{equation*}\]</span>
To apply Fermat’s theorem we take the derivative and identify any values of <span class="math inline">\(m\)</span> for which the derivative is zero:
<span class="math display">\[\begin{eqnarray*}
    \frac{d}{dm} \mbox{SSE}(m) &amp;=&amp; 2(3-2m)(-2)+2(4-5m)(-5) \\
    &amp;=&amp; -4(3-2m) - 10(4-5m) = -12 +8m+-40+50m = -52+58m.
\end{eqnarray*}\]</span>
<p>
The only local extremum (and hence the only candidate for global minimum) is <span class="math inline">\(m=52/58 = 26/29\)</span>. We conclude that the best fit line is <span class="math inline">\(y=\frac{26}{29}x\)</span>.
</p>
<p>
The analysis in Example 3 can be extended to determine the general solution of the linear regression task. For details, see, e.g. (Fairway, 2002). The solution formula is as follows:
</p>
<p>
<strong>Linear regression formula</strong> The best fit line <span class="math inline">\(y=mx+b\)</span> to the dataset <span class="math inline">\((x_1, y_1)\)</span>, <span class="math inline">\((x_2, y_2)\)</span>, <span class="math inline">\(\ldots\)</span>, <span class="math inline">\((x_n, y_n)\)</span> is given by
<span class="math display">\[\begin{eqnarray*}
    m&amp;=&amp; \frac{\sum_{i=1}^n (x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^n (x_i-\bar{x})^2} \\
    b&amp;=&amp; \bar{y}-m\bar{x}, \\
\end{eqnarray*}\]</span>
where <span class="math inline">\(\bar{x}\)</span> and <span class="math inline">\(\bar{y}\)</span> are the averages
<span class="math display">\[\begin{eqnarray*}
    \bar{x}=\frac{1}{n} \sum_{i=1}^n x_i \ \ \  \ \ \ \ \ \ \ \ \ \  \ \ \
\bar{y}=\frac{1}{n} \sum_{i=1}^n y_i
\end{eqnarray*}\]</span>
</p>
<p>
This formula is somewhat unwieldy, but is straightforward to implement. In R, the command <code>lm</code> implements this formula, as the following example illustrates.
</p>
</div>
<div id="example-4" class="section level4">
<h4><span class="header-section-number">8.3.1.2</span> Example 4</h4>
<p>We’ll work with a dataset called <strong>iris</strong> that is included in R, shown below (via the <code>plot</code> command).</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" data-line-number="1"><span class="kw">plot</span>(iris<span class="op">$</span>Sepal.Length<span class="op">~</span>iris<span class="op">$</span>Petal.Width, <span class="dt">xlab =</span> <span class="st">&quot;Petal Width&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Sepal Length&quot;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-iris-dataset-1.png" title="Dataset: Sepal Length vs Petal Width" alt="Dataset: Sepal Length vs Petal Width" width="672" /></p>
<p>The <code>lm</code> command applies the formula described above to arrive at the line of best fit (i.e. the minimizer of the sum of squared errors). The command below generates the parameters (<span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span>) that specify the line <span class="math inline">\(y=mx+b\)</span>.</p>
<div class="sourceCode" id="cb84"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb84-1" data-line-number="1"><span class="kw">lm</span>(iris<span class="op">$</span>Sepal.Length<span class="op">~</span>iris<span class="op">$</span>Petal.Width)</a></code></pre></div>
<pre><code>
Call:
lm(formula = iris$Sepal.Length ~ iris$Petal.Width)

Coefficients:
     (Intercept)  iris$Petal.Width  
          4.7776            0.8886  </code></pre>
<p>The best-fit line has intercept <span class="math inline">\(b=4.7776\)</span> and slope is <span class="math inline">\(m=0.8886\)</span>.</p>
<p>We can plot the data together with the line of best fit with the <code>abline</code> command:
<!-- or __abline(lm(iris$Sepal.Length~iris$Petal.Width))__.  --></p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="kw">plot</span>(iris<span class="op">$</span>Sepal.Length<span class="op">~</span>iris<span class="op">$</span>Petal.Width, <span class="dt">xlab =</span> <span class="st">&quot;Petal Width&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Sepal Length&quot;</span>)</a>
<a class="sourceLine" id="cb86-2" data-line-number="2"><span class="kw">abline</span>(<span class="kw">lm</span>(iris<span class="op">$</span>Sepal.Length<span class="op">~</span>iris<span class="op">$</span>Petal.Width))</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-linear_regression_iris_dataset-1.png" title="Dataset: Sepal Length vs Petal Width, with best-fit line" alt="Dataset: Sepal Length vs Petal Width, with best-fit line" width="672" /></p>
<!-- Here is a link to a linear regression example where you implement values of X and Y and a plot with those values and a fitted line will be produced.  -->
<!-- https://munkmuni.shinyapps.io/LinearRegression/ -->
</div>
</div>
<div id="nonlinear-regression" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Nonlinear Regression</h3>
<p>
Nonlinear regression is the task of fitting a nonlinear model (e.g. a curve) to a dataset. The setup for this task is identical to linear regression. We begin by selecting a parameterized family of models (i.e. curves), and aim to identify the model (i.e. curve) that minimizes the sum of squared errors when compared to the data. The choice of model family is typically based on the mechanism that produced the data. For example, if we are investigating the rate law for a single-substrate enzyme-catalysed reaction, we might choose the family of curves specified by Michaelis-Menten kinetics, which relate reaction velocity <span class="math inline">\(V\)</span> to substrate concentration <span class="math inline">\(S\)</span>:
<span class="math display">\[\begin{eqnarray*}
V = \frac{V_{\mbox{max}} S}{K_M+S}
\end{eqnarray*}\]</span>
Our goal would then be to identify values for the parameters <span class="math inline">\(V_{\mbox{max}}\)</span> and <span class="math inline">\(K_M\)</span> that minimize the sum of squared errors when comparing with observed data.
</p>
<p>
<strong>Regression via linearizing transformation:</strong>
In several important cases, the nonlinear regression task can be transformed into a linear regression task. In the case of Michaelis-Menten kinetics, several linearizing transformations have been proposed, e.g. Eadie–Hofstee and Lineweaver–Burk (Cho and Lim, 2018). Another common example is fitting exponential curves (to describe e.g. population growth or drug clearance). In those cases, application of a logarithm transforms the data so that a linear trend is captured. For example, the relationship <span class="math inline">\(y = e^{rt}\)</span> becomes, after applying a logarithm, <span class="math inline">\(Y = \ln(y) = \ln (e^{rt})= rt\)</span>. Linear regression on the transformed data <span class="math inline">\((t_i, Y_i)\)</span> then provides an estimate of the value of <span class="math inline">\(r\)</span>.
</p>
<p>
<p>Unfortunately, linearizing transformations are only available in a handful of special cases. Moreover, they often introduce biases that can make interpretation of the resulting model difficult.</p>
<p>In general, the nonlinear regression task must be addressed directly. An example of the general procedure in R follows.</p>
<div id="example-5" class="section level4">
<h4><span class="header-section-number">8.3.2.1</span> Example 5</h4>
<p>We begin by defining a dataset against which we will fit a Michaelis-Menten curve.</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="co">#substrate concentrations</span></a>
<a class="sourceLine" id="cb87-2" data-line-number="2">S &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">4.32</span>, <span class="fl">2.16</span>, <span class="fl">1.08</span>, <span class="fl">0.54</span>, <span class="fl">0.27</span>, <span class="fl">0.135</span>, <span class="fl">3.6</span>, <span class="fl">1.8</span>, <span class="fl">0.9</span>, <span class="fl">0.45</span>, <span class="fl">0.225</span>, <span class="fl">0.1125</span>, <span class="fl">2.88</span>, <span class="fl">1.44</span>, <span class="fl">0.72</span>, <span class="fl">0.36</span>, <span class="fl">0.18</span>, <span class="fl">0.9</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb87-3" data-line-number="3"></a>
<a class="sourceLine" id="cb87-4" data-line-number="4"><span class="co">#reaction velocities</span></a>
<a class="sourceLine" id="cb87-5" data-line-number="5">V &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.48</span>, <span class="fl">0.42</span>, <span class="fl">0.36</span>, <span class="fl">0.26</span>, <span class="fl">0.17</span>, <span class="fl">0.11</span>, <span class="fl">0.44</span>, <span class="fl">0.47</span>, <span class="fl">0.39</span>, <span class="fl">0.29</span>, <span class="fl">0.18</span>, <span class="fl">0.12</span>, <span class="fl">0.50</span>, <span class="fl">0.45</span>, <span class="fl">0.37</span>, <span class="fl">0.28</span>, <span class="fl">0.19</span>, <span class="fl">0.31</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb87-6" data-line-number="6">MM_data=<span class="kw">cbind</span>(S,V)</a>
<a class="sourceLine" id="cb87-7" data-line-number="7"><span class="kw">plot</span>(MM_data)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-Michaelis-Menten-dataset-1.png" title="Dataset: reaction velocity against substrate concentration" alt="Dataset: reaction velocity against substrate concentration" width="672" /></p>
The <code>nls</code> command can be used for nonlinear regression in R. Like the <code>lm</code> command, the <code>nls</code> function takes as inputs the dataset and the model. In addition, <code>nls</code> requires that the user provide ‘guesses’ for the values of the parameters to be estimated. In this case, we can roughly estimate <span class="math inline">\(V_{\mbox{max}} \approx 0.5\)</span> as the maximal <span class="math inline">\(V\)</span>-value that can be achieved, and <span class="math inline">\(K_M \approx 0.3\)</span> as the <span class="math inline">\(S\)</span>-value at which <span class="math inline">\(V\)</span> reaches half of its maximal value.
</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1">MMmodel.nls &lt;-<span class="st"> </span><span class="kw">nls</span>(V <span class="op">~</span><span class="st"> </span>Vmax<span class="op">*</span>S<span class="op">/</span>(Km<span class="op">+</span>S),<span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">Km=</span><span class="fl">0.3</span>, <span class="dt">Vmax=</span><span class="fl">0.5</span>)) <span class="co">#perform the nonlinear regression analysis</span></a>
<a class="sourceLine" id="cb88-2" data-line-number="2">params &lt;-<span class="st"> </span><span class="kw">summary</span>(MMmodel.nls)<span class="op">$</span>coeff[,<span class="dv">1</span>] <span class="co">#extract the parameter estimates</span></a>
<a class="sourceLine" id="cb88-3" data-line-number="3">params</a></code></pre></div>
<pre><code>       Km      Vmax 
0.4187090 0.5331688 </code></pre>
<!-- #summary(MMmodel.nls) #report on the results -->
<p>We thus arrive at parameter estimates that characterize the best-fit model: <span class="math inline">\(K_M = 0.4187090\)</span> and <span class="math inline">\(V_{\mbox{max}} = 0.5331688\)</span>). Further details on using the <code>nls</code> command can be found in (Ritz and Streibig, 2008). We can now plot the best fit model along with the dataset:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">plot</span>(MM_data)</a>
<a class="sourceLine" id="cb90-2" data-line-number="2"><span class="kw">curve</span>((params[<span class="dv">2</span>]<span class="op">*</span>x)<span class="op">/</span>(params[<span class="dv">1</span>]<span class="op">+</span>x), <span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="fl">4.5</span>, <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;firebrick&#39;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot_nonlinear_regression_Michaelis-Menten_dataset-1.png" title="Dataset: reaction velocity against substrate concentration and best-fit model" alt="Dataset: reaction velocity against substrate concentration and best-fit model" width="672" /></p>
<p>
From the implementation of <code>nls</code>, the reader might have the mistaken impression that nonlinear regression and linear regression are very similar tasks. Although the problem set-up is similar in both cases (chose model type, then minimize SSE), the strategy for optimization is very different. (The first hint of this is the need to provide a set of ‘guesses’ to <code>nls</code>.) As we saw above, the solution to the linear regression task can be stated as a general formula. For nonlinear regression, no such formula exists. Worse, as we’ll discuss in the next section there is no procedure (algorithm) that is guaranteed to find the solution!
</p>
</div>
</div>
</div>
<div id="iterative-optimization-algorithms" class="section level2">
<h2><span class="header-section-number">8.4</span> Iterative Optimization Algorithms</h2>
<p>
The best techniques for addressing the general nonlinear regression task are iterative global optimization routines. As we’ll discuss below, these algorithms start with a ‘guess’ for the parameter values and then take steps through parameter space to improve the quality of that estimate. In the exercise above, the <code>nls</code> command executed this kind of algorithm, which is why it requires that the user supply an initial ‘guess’.
</p>
<div id="gradient-descent" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Gradient Descent</h3>
<p>
A simple iterative optimization algorithm is <strong>gradient descent</strong>, which can be understood intuitively via a thought experiment. Imagine finding your way to a valley bottom in a thick fog. The fog obscures your vision so that you can only discern changes in elevation in your immediate vicinity. To make your way to the valley bottom, it would be reasonable to take each step of your journey in the direction of steepest decline. This strategy is guaranteed to lead to a local minimum, but cannot guarantee arrival at the lowest point: the global minimum. That is, the search may lead to a local minimum at which there is no direction of local descent, and so the algorithm gets `stuck’.
</p>
<p>
Mathematically, the local change in ‘elevation’ is determined by evaluating the objective function at points near the current estimate to determine the direction of steepest descent. (Technically, this involves a linearization of the function at the current position, or equivalently a calculation of the <em>gradient vector</em>). A step is then taken in this direction, and the process is repeated from this updated estimate. To implement this algorithm, a number of details have to be specified: how long should each step be? How many steps should be taken? Or should there be some other ‘termination condition’ that will trigger the end of the journey? (Each of these decisions involve a trade-off between precision and computation time. For instance, taking very small steps will guarantee a smooth path down the steepest route, but might take a great many step to complete the journey.) Termination conditions are often specified in terms of the local topography: the algorithm stops when the current estimate is at a sufficiently flat position (no downhill direction detected).
</p>
<p>
To illustrate the gradient descent approach, consider the following algorithm, which incorporates two termination conditions: a maximum number of allowed iterations and a threshold for shallowness.
</p>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1"><span class="kw">library</span>(numDeriv)       <span class="co"># contains functions for calculating gradient</span></a>
<a class="sourceLine" id="cb91-2" data-line-number="2"></a>
<a class="sourceLine" id="cb91-3" data-line-number="3"><span class="co">#define function that implements gradient descent. Inputs are the objective function f, the initial parameter estimate x0,  the size of each step, the maximum number of iterations to be applied, and a threshold gradient below which the landscape is considered flat (at which iterations are terminated)</span></a>
<a class="sourceLine" id="cb91-4" data-line-number="4">grad.descent =<span class="st"> </span><span class="cf">function</span>(f, x0, <span class="dt">step.size=</span><span class="fl">0.05</span>, <span class="dt">max.iter=</span><span class="dv">200</span>,</a>
<a class="sourceLine" id="cb91-5" data-line-number="5">                        <span class="dt">stopping.deriv=</span><span class="fl">0.01</span>, ...) {</a>
<a class="sourceLine" id="cb91-6" data-line-number="6">  n =<span class="st"> </span><span class="kw">length</span>(x0)  <span class="co"># record the number of parameters to be estimated (i.e. the dimension of the parameter space)</span></a>
<a class="sourceLine" id="cb91-7" data-line-number="7">  xmat =<span class="st"> </span><span class="kw">matrix</span>(<span class="dv">0</span>,<span class="dt">nrow=</span>n,<span class="dt">ncol=</span>max.iter) <span class="co"># initialize a matrix to record the sequence of estimates</span></a>
<a class="sourceLine" id="cb91-8" data-line-number="8">  xmat[,<span class="dv">1</span>] =<span class="st"> </span>x0   <span class="co"># the first row of matrix xmat is the initial &#39;guess&#39;</span></a>
<a class="sourceLine" id="cb91-9" data-line-number="9">  <span class="cf">for</span> (k <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span>max.iter) { <span class="co">#loop over the allowed number of steps</span></a>
<a class="sourceLine" id="cb91-10" data-line-number="10">     <span class="co"># Calculate the gradient (a vector indicating steepness and direct of greatest ascent)</span></a>
<a class="sourceLine" id="cb91-11" data-line-number="11">    grad.current =<span class="st"> </span><span class="kw">grad</span>(f,xmat[,k<span class="dv">-1</span>],...)</a>
<a class="sourceLine" id="cb91-12" data-line-number="12">    <span class="co"># Check termination condition: has a flat valley bottom been reached?</span></a>
<a class="sourceLine" id="cb91-13" data-line-number="13">    <span class="cf">if</span> (<span class="kw">all</span>(<span class="kw">abs</span>(grad.current) <span class="op">&lt;</span><span class="st"> </span>stopping.deriv)) {</a>
<a class="sourceLine" id="cb91-14" data-line-number="14">      k =<span class="st"> </span>k<span class="dv">-1</span>; <span class="cf">break</span></a>
<a class="sourceLine" id="cb91-15" data-line-number="15">    }</a>
<a class="sourceLine" id="cb91-16" data-line-number="16">    <span class="co"># Step in the opposite direction of the gradient</span></a>
<a class="sourceLine" id="cb91-17" data-line-number="17">    xmat[,k] =<span class="st"> </span>xmat[,k<span class="dv">-1</span>] <span class="op">-</span><span class="st"> </span>step.size <span class="op">*</span><span class="st"> </span>grad.current</a>
<a class="sourceLine" id="cb91-18" data-line-number="18">  }</a>
<a class="sourceLine" id="cb91-19" data-line-number="19">  xmat =<span class="st"> </span>xmat[,<span class="dv">1</span><span class="op">:</span>k] <span class="co"># Trim any unused columns from xmat</span></a>
<a class="sourceLine" id="cb91-20" data-line-number="20">  <span class="kw">return</span>(<span class="kw">list</span>(<span class="dt">x=</span>xmat[,k], <span class="dt">xmat=</span>xmat, <span class="dt">k=</span>k))</a>
<a class="sourceLine" id="cb91-21" data-line-number="21">}</a></code></pre></div>
<div id="example-6" class="section level4">
<h4><span class="header-section-number">8.4.1.1</span> Example 6</h4>
<p>
We’ll first demonstrating the performance of this algorithm on a simple function with a single local minimum. (Here and below, code for generating plots is suppressed to avoid clutter. All code can be accessed in the .rmd file posted at the associated github repository.)
</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1">Paraboloid =<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb92-2" data-line-number="2">  <span class="kw">return</span>((x[<span class="dv">1</span>]<span class="op">-</span><span class="dv">3</span>)<span class="op">^</span><span class="dv">2</span> <span class="op">+</span><span class="st"> </span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span><span class="op">*</span>(x[<span class="dv">2</span>])<span class="op">^</span><span class="dv">2</span><span class="op">+</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb92-3" data-line-number="3">}</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-wireframe-convex-function-1.png" title="Surface plot of paraboloid" alt="Surface plot of paraboloid" width="672" /></p>
<p>
In this case, we expect that the minimum (valley bottom) will be reached from any initial ‘guess’. Starting from four distinct points, the algorithm follows the paths shown below.
</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="co">#define a set of initial guess values</span></a>
<a class="sourceLine" id="cb93-2" data-line-number="2">x0 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.9</span>,<span class="op">-</span><span class="fl">1.9</span>)</a>
<a class="sourceLine" id="cb93-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="dv">3</span>,<span class="fl">1.1</span>)</a>
<a class="sourceLine" id="cb93-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="op">-</span><span class="dv">5</span>)</a>
<a class="sourceLine" id="cb93-5" data-line-number="5">x3 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">5</span>,<span class="dv">7</span>)</a>
<a class="sourceLine" id="cb93-6" data-line-number="6"></a>
<a class="sourceLine" id="cb93-7" data-line-number="7"><span class="co">#run the gradient descent algorithm from each</span></a>
<a class="sourceLine" id="cb93-8" data-line-number="8">gd =<span class="st"> </span><span class="kw">grad.descent</span>(simpleFun,x0,<span class="dt">step.size=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb93-9" data-line-number="9">gd1 =<span class="st"> </span><span class="kw">grad.descent</span>(simpleFun,x1,<span class="dt">step.size=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb93-10" data-line-number="10">gd2 =<span class="st"> </span><span class="kw">grad.descent</span>(simpleFun,x2,<span class="dt">step.size=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a>
<a class="sourceLine" id="cb93-11" data-line-number="11">gd3 =<span class="st"> </span><span class="kw">grad.descent</span>(simpleFun,x3,<span class="dt">step.size=</span><span class="dv">1</span><span class="op">/</span><span class="dv">3</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-paths-gradient-descent-convex-function-1.png" width="672" /></p>
<p>
The table below shows the estimates and the corresponding objective values reached by gradient descent starting from those four points. Each run has arrived at essentially the same point: <span class="math inline">\((3, 0)\)</span>, where the function reaches its mimimum value of 2.
</p>
<pre><code>       x1          x2 objective value
x0_out  3 -0.01246994        2.000052
x1_out  3  0.01193417        2.000047
x2_out  3 -0.01200889        2.000048
x3_out  3  0.01307634        2.000057</code></pre>
</div>
<div id="example-7" class="section level4">
<h4><span class="header-section-number">8.4.1.2</span> Example 7</h4>
<p>
Next, let’s consider a function that has multiple local minima. The second plot below is interactive, allowing you to rotate the surface.
</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1">Example_<span class="dv">7</span>_function =<span class="st"> </span><span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb95-2" data-line-number="2">  <span class="kw">return</span>((<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">^</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>x[<span class="dv">2</span>]<span class="op">^</span><span class="dv">2</span><span class="op">+</span><span class="dv">3</span>)<span class="op">+</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>x[<span class="dv">1</span>]<span class="op">+</span><span class="dv">1</span><span class="op">-</span><span class="kw">exp</span>(x[<span class="dv">2</span>])))</a>
<a class="sourceLine" id="cb95-3" data-line-number="3">}</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-wireframe-complicated-function-1.png" title="Surface plot with multiple minima" alt="Surface plot with multiple minima" width="672" /></p>
<div id="htmlwidget-07c8b5fecaae8da83b41" style="width:672px;height:480px;" class="plotly html-widget"></div>
<script type="application/json" data-for="htmlwidget-07c8b5fecaae8da83b41">{"x":{"visdat":{"2130504e559c":["function () ","plotlyVisDat"]},"cur_data":"2130504e559c","attrs":{"2130504e559c":{"x":[-4,-3.72413793103448,-3.44827586206897,-3.17241379310345,-2.89655172413793,-2.62068965517241,-2.3448275862069,-2.06896551724138,-1.79310344827586,-1.51724137931034,-1.24137931034483,-0.96551724137931,-0.689655172413793,-0.413793103448276,-0.137931034482759,0.137931034482759,0.413793103448276,0.689655172413793,0.96551724137931,1.24137931034483,1.51724137931035,1.79310344827586,2.06896551724138,2.3448275862069,2.62068965517241,2.89655172413793,3.17241379310345,3.44827586206897,3.72413793103448,4],"y":[-4,-3.72413793103448,-3.44827586206897,-3.17241379310345,-2.89655172413793,-2.62068965517241,-2.3448275862069,-2.06896551724138,-1.79310344827586,-1.51724137931034,-1.24137931034483,-0.96551724137931,-0.689655172413793,-0.413793103448276,-0.137931034482759,0.137931034482759,0.413793103448276,0.689655172413793,0.96551724137931,1.24137931034483,1.51724137931035,1.79310344827586,2.06896551724138,2.3448275862069,2.62068965517241,2.89655172413793,3.17241379310345,3.44827586206897,3.72413793103448,4],"z":[[15.7417433483319,15.2051294365808,14.705283766665,14.241771601612,13.8140004216652,13.4211561412909,13.0621101177958,12.7352825992514,12.4384420699128,12.1684135925778,11.920668331915,11.6887888103448,11.4639002380353,11.2344570420108,10.9875799516382,10.715054934603,10.4308255619518,10.2106711450234,10.2532619437596,10.8752501597338,12.1101230061412,12.7078318169249,11.3672575023616,12.5266756483111,12.3978392858193,14.0972791632269,14.3681027131981,14.7063710622763,14.2110040656083,15.3307453710245],[14.9178298418846,14.3840529060493,13.8879327785247,13.4293073375634,13.0079357352367,12.6234536179562,12.2752999772785,11.9625968518469,11.6839510106344,11.4371281350077,11.218524065598,11.0223299920848,10.8392880343381,10.6550620720874,10.4488091946874,10.1943208600946,9.87077259944913,9.49937363138537,9.22840712062919,9.43466123887695,10.5224378188849,11.732359961777,10.7789815537373,10.9206497511802,11.8765405192537,12.8724471495259,12.9019041599986,13.8882442985131,12.6769859879092,13.7216149019558],[13.8782374942126,13.3476173683145,12.8556592814969,12.4025218378162,11.9883873342051,11.6134492730831,11.2778807627988,10.9817661727441,10.7249640060801,10.5068436098878,10.3257950561701,10.1783420003624,10.0575908022069,9.95066912182415,9.83495566440898,9.67402491659634,9.41846415276455,9.02861853894907,8.55838024673625,8.33285932039226,9.00719327455935,10.5380265214737,10.3338054479316,9.50601517157815,11.364590769354,11.4743384528593,11.3778943540983,12.8551025804806,11.4341910126712,12.2517893837105],[12.6378913128694,12.1098110290853,11.6212163713254,11.1725396027668,10.764332067521,10.3972877010383,10.072262357543,9.79027770006393,9.55248595596649,9.36004728799454,9.21382375508853,9.113703104057,9.05720166265094,9.03673081365999,9.0346038798909,9.01498786116519,8.91454529093677,8.64452508695893,8.14843561000593,7.60323046645783,7.72046695692153,9.18567682943478,9.87025769003905,8.40899289675646,10.7203340113264,10.0242699078922,9.9547520219823,11.6199566535928,10.5578457177423,11.0638680379462],[11.2938742673512,10.7669631448296,10.2799351871989,9.83336804012634,9.42801848030683,9.06487494681101,8.74522205998312,8.47071559199947,8.24345956420787,8.0660606699236,7.94159713398321,7.87335410636069,7.86399492710744,7.91346666830121,8.01426589544395,8.1417857742923,8.23755986166757,8.19007823069024,7.84922965624729,7.19129928736805,6.7731054126543,7.80563155318415,9.25490176613425,7.68413368721615,9.86396018580973,8.68156082428971,8.78380596189133,10.2783462813677,10.0370136768938,10.2393764570438],[9.99661734908962,9.46915775152429,8.98143483765151,8.53400021589278,8.12759151760803,7.76319836040277,7.44215291212753,7.16625369401145,6.93793214072249,6.76046792463336,6.63824195522113,6.5769618981228,6.58364774392537,6.66579796066577,6.82831904469103,7.06513254992613,7.33999331612797,7.55172379592247,7.50113956386339,6.97089483260019,6.19782978918821,6.55901327271268,8.42193421058404,7.29812981891621,8.80118930159828,7.59625404812156,7.96412733630847,8.97998826083662,9.77784090838005,9.7745734069102],[8.90525829336918,8.37569534763658,7.88522205348719,7.43420028906563,7.02312964396913,6.65270723951544,6.32391746244656,6.03816786133218,5.79749574167692,5.60488049291869,5.46470564862004,5.38340760686196,5.37027978543024,5.43814683399748,5.60286914821769,5.87870048467052,6.26237292971106,6.69306634545322,6.9816322513904,6.78159255961763,5.93952564248827,5.58993241084884,7.39271217001663,7.13969934553275,7.6215739895183,6.86458986819002,7.51312972403426,7.88434708583389,9.63140759391217,9.58156106011505],[8.14041652399345,7.60781953786221,7.11335663086319,6.65709256140444,6.23914062118043,5.85969856038565,5.51911086212039,5.21797640400999,4.95733378005065,4.73897794547356,4.56599421865892,4.44363769129031,4.38071733217425,4.39157818268905,4.49832787245925,4.73132247494145,5.12124109560971,5.6656697383289,6.24162678734347,6.47632125679516,5.87159847123694,4.98272162094124,6.26941554492811,7.05260966803163,6.47192423454154,6.50044614645578,7.36140153865146,7.11331292414904,9.43792090725641,9.51436919117438],[7.74839863256208,7.2127372830656,6.71422947163219,6.25262361949633,5.82761310047564,5.43883761011253,5.08589950048719,4.7684112578365,4.48610455579475,4.23905725190489,4.02814086305857,3.85586969529529,3.72795415533719,3.65600270119165,3.6618096278341,3.78282498294353,4.07457068681467,4.59375525256004,5.32005944419999,5.96500728715905,5.83354137681731,4.73690957787235,5.20472450553816,6.88204047379872,5.51274065258502,6.43121998119075,7.37330432347689,6.71502999608069,9.07413447164544,9.41227269620248],[7.68745662610326,7.14960995805766,6.6482022996951,6.18274039254918,5.75258868567678,5.35693574242704,4.99475978821011,4.66480191485404,4.36556650305233,4.0953912079777,3.85267511728942,3.63644561633435,3.44761995309818,3.29162512428974,3.18347450550856,3.15659859758743,3.27488568049136,3.63733819305912,4.33232913141433,5.24130446790056,5.67856642588778,4.76736084654861,4.35651075672414,6.52052757150049,4.87058499236444,6.51937304344629,7.387224258288,6.6496094949859,8.48992244418463,9.14748770917851],[7.8401768885896,7.30167245289424,6.79937055167422,6.33268262410214,5.90083279794021,5.50279930292466,5.13723945539171,4.80239654113655,4.49599148170395,4.21511504046355,3.95616895595909,3.71498232194167,3.48740702894665,3.27107811931414,3.06977274209525,2.90298115428514,2.82399877411794,2.94474241237796,3.43605209291495,4.38447733730667,5.31716482963173,4.92954032107305,3.84098818462826,5.93985351949499,4.60052219493744,6.60324688200591,7.26353200736939,6.80096682436,7.72315465341834,8.66309221287396],[8.04832335942074,7.51088390337891,7.00995873518043,6.54503865051079,6.11543770046997,5.7202269974936,5.35814139808797,5.02744769724828,4.72575968725528,4.44978457269081,4.19499454815246,3.95525815666227,3.72259290709264,3.48754370245686,3.24153057414638,2.98431968736196,2.74279787098801,2.60858433843345,2.78428836324492,3.53587893861248,4.74366920127094,5.06240486425145,3.69822570504768,5.19942131696513,4.66977954981919,6.54503601594075,6.92601854297473,7.01127038198912,6.8884585058301,7.98991798097136],[8.15979501151168,7.62482724347478,7.1271412305864,6.66645823440874,6.24238578873456,5.85436354152598,5.50157932820359,5.18283779170265,4.89635371616136,4.63942790370657,4.40794665777131,4.19563751643098,3.99305245790657,3.78645210131276,3.55744551633597,3.28614416822929,2.96504747252601,2.63828850688349,2.48012030553246,2.85700347623359,4.03793538629725,5.03619380210665,3.88025657790787,4.4286262158689,4.9674721888341,6.27168251847763,6.38451062081922,7.12777901264259,6.14320263136652,7.23740106858057],[8.07376272224031,7.54193986266262,7.04839418905645,6.59316014612519,6.17625521700086,5.79765387292242,5.45723794282219,5.15470464678801,4.88939953408211,4.66001800715983,4.46408124184803,4.29703716110194,4.15077596831005,4.01135124046518,3.85601870187175,3.65113715704826,3.3570444505633,2.95729077495215,2.54606231627296,2.48156196441109,3.34164477244736,4.7909361641491,4.26531281841691,3.78845692179402,5.33750882042146,5.79655638481541,5.73195463274617,7.0481688407982,5.64079682047769,6.56110602169904],[7.77059090634337,7.24165290566792,6.75192008537579,6.30172985164407,5.89150444872216,5.52176074693527,5.19310963668414,4.90623076692212,4.66179466412634,4.46027847804564,4.30157386249751,4.1841999808507,4.10379189132093,4.05033481151865,4.0034806174377,3.92581867472489,3.75729523398898,3.42575917938775,2.91737954625711,2.47580270032263,2.81625950778067,4.35424742023961,4.69396055999118,3.42372125234179,5.62491273494508,5.21548790771494,5.11683422037449,6.75089848021905,5.48516676997314,6.11656118673425],[7.31766449088828,6.79049520077645,6.30311626126976,5.85606791275906,5.45005185715623,5.08597397957534,4.76499294657995,4.48856914617995,4.25849909940665,4.0769001063188,3.946066451089,3.86802798444604,3.84345994148158,3.86925112577441,3.93348799002627,4.00609228594554,4.02443997216797,3.88208989083344,3.46129834635533,2.81885169374611,2.59510766223578,3.83313442810909,5.01641298828722,3.42007356882482,5.72181205128644,4.67832947665253,4.69910688448253,6.30160156653038,5.69991352359277,6.01310476246663],[6.84939204335754,6.32235044024808,5.83516788108375,5.38842712096793,4.9829010853213,4.61961584733168,4.29993400863404,4.02566331759646,3.7991931616626,3.6236526700034,3.50305791487615,3.44234706427301,3.44703515019191,3.52183782794651,3.66681154190866,3.86813632625735,4.07920189884444,4.19086409702575,4.01640762581782,3.40890714595882,2.74381734170696,3.38224052137369,5.13698045595917,3.77859634269856,5.59945132958808,4.34448600325758,4.60273565946099,5.83360959130024,6.22130972399858,6.28143804894887],[6.52731924174838,5.99872641021397,5.50952445393174,5.06016570672629,4.65126494737999,4.2836639439241,3.95852487304638,3.67746638355328,3.44276166457743,3.25762308698697,3.12659671815133,3.05606361304164,3.05474198250415,3.13377493524401,3.30517215710515,3.57547312056573,3.92791317701087,4.28303432875269,4.44055853916232,4.0934497299652,3.24084116310489,3.15795419422872,5.04246694615437,4.41547892581466,5.31672491650943,4.33561057983307,4.87890235655796,5.50838500072737,6.91721107944332,6.86451465683631],[6.49218924813086,5.96082660886793,5.46798874593392,5.01386333916198,4.59872458149051,4.22298013619902,3.88724687228676,3.59247399548156,3.34014391441135,3.132598951765,2.97356609789682,2.86897567930762,2.82816190554336,2.86538855590491,3.00105461306937,3.26011836104137,3.66063579243986,4.17641470089161,4.6530479889211,4.71450359439364,3.98385056653459,3.27199989951598,4.8060860678299,5.18688885130569,5.0026997027936,4.69950321144911,5.49081906151681,5.46760626872315,7.62627095657998,7.63446935765987],[6.82217635068928,6.28764728106601,5.79063597854804,5.33100977388697,4.90862090581625,4.52332195639696,4.17500142745009,3.8636573992637,3.58954158651524,3.35343116573274,3.1571280440313,3.00435196030652,2.90228314143213,2.86407317722573,2.91245682957048,3.08340467698853,3.4244351317329,3.97039446944641,4.65856774265406,5.15551662297161,4.82010241498731,3.75828316274925,4.56573448833592,5.93165496084252,4.81831376723637,5.39592588437809,6.32464516367225,5.7911239796841,8.20582093923759,8.43056283953667],[7.5096872856803,6.97253481982641,6.47205103744102,6.00782229057096,5.5793227127663,5.1858935520068,4.82672803067304,4.50087367329913,4.20727680280668,3.94491881787471,3.71314181588534,3.51235109381772,3.34544212147963,3.22055148787186,3.1560030602918,3.18810523185103,3.37973746719014,3.81644368750667,4.54581212343624,5.37594795790848,5.59176641517007,4.56282891876561,4.48307028944288,6.51909589017047,4.90861705886719,6.30854343242294,7.22327022415744,6.47326469972759,8.57420851681427,9.10688203974301],[8.46361458293769,7.92516026318482,7.42293543939448,6.95636823736237,6.52471073940528,6.12698834794326,5.76193842190199,5.42794058273664,5.12294844645951,4.84444990981061,4.58952249689557,4.35513678852389,4.1390442498074,3.94195188847178,3.77233496224864,3.65606495045221,3.65272252056773,3.87316357953016,4.46115744982518,5.42329873697381,6.18276218176811,5.55979892855245,4.6955399844213,6.88780052961405,5.359726974727,7.27944667909389,8.03293741914796,7.42451460143716,8.73502794520049,9.57564070098241],[9.53637324144289,8.99832494289478,8.49660718039962,8.03065956052277,7.59973406330225,7.20282946314042,6.8386019516447,6.50524410401827,6.20032410580226,5.92058183315846,5.66169749876047,5.41810596514229,5.18308195862203,4.94969309696532,4.71405094594956,4.48391255802414,4.29787889135952,4.25922069874032,4.56522400571007,5.419015852555,6.55320677190216,6.58883501396084,5.27559045992955,7.06385213068094,6.17327251773437,8.15601291092183,8.64887177819299,8.49808321660994,8.77605363028477,9.83323045757803],[10.5676974430779,10.0316425516669,9.53252947975687,9.06997561471787,8.64345481361554,8.25223634239686,7.8952940083648,7.57116970436877,7.27776784066738,7.01204773369425,6.76957420688971,6.54389789166144,6.32581253990229,6.10280360446274,5.85977287480388,5.58406102574731,5.28183424564861,5.01813193218979,4.98521102999322,5.52245187478394,6.75125018210199,7.50264628660985,6.20916993793158,7.15308794835526,7.26591088052513,8.83619756189682,9.04637277057326,9.53346436774132,8.84319255873602,9.96129163912876],[11.4322003731495,10.8991347195605,10.4039510103335,9.94655795618282,9.52680742720013,9.14445603048862,8.79909971774216,8.49006240129635,8.2162065946611,7.97561339520557,7.76504840176658,7.57909190254212,7.40878868306503,7.23975579645995,7.05016573830953,6.81069953472528,6.49325875172565,6.10535060956203,5.77714959718539,5.88357624270354,6.89878655472539,8.21071858199106,7.39989792315852,7.30969141558051,8.49405991184324,9.29881653631073,9.28814433251237,10.4040673166052,9.09718554484038,10.1024860243699],[12.0765834922904,11.546615833371,11.0555200975265,10.6035234094463,10.1908996106661,9.81796487771363,9.4850572103757,9.19248317344973,8.9404009394774,8.72858283236198,8.5559550538323,8.41973588010801,8.31387864272365,8.22640021883302,8.13522124996354,8.00306369079788,7.77590285993017,7.40148598858678,6.90927506775863,6.59847095456066,7.1552813770356,8.70617506810896,8.69766667977311,7.69043730617271,9.69650696117394,9.60983271327629,9.50568710672011,11.0547833075179,9.66590645208122,10.4181611224781],[12.5354531902933,12.0077729348214,11.5197106530065,11.0717444829301,10.6644889734923,10.2987260434079,9.9754354371398,9.69581539704741,9.4612728160757,9.27333880887767,9.13341883600256,9.04219465452159,8.99832252626559,8.99577521433122,9.01877368518038,9.03314330042696,8.97496527197858,8.74773315731916,8.27145343103522,7.68081767188192,7.67044916354755,9.06781698566698,9.94318658146074,8.40816790214701,10.7422496773291,9.9027810758738,9.86027485620587,11.5183394134694,10.6064148749375,11.040469354388],[12.9210511371807,12.3941688935595,11.9071853511646,11.4606877711931,11.0554490389222,10.6924847762943,10.3731257275386,10.0991062271356,9.87266438470946,9.69663573841868,9.57448792898033,9.51016387891194,9.50742154251527,9.5679781866945,9.68703762349782,9.84367014564063,9.98303113321069,9.99299932219743,9.70786421813153,9.05783760625475,8.5398247966681,9.43673891819225,11.0152562413163,9.49830638538668,11.569371723705,10.3391414225405,10.4950955302026,11.9055865854658,11.8880233699097,12.0331510475986],[13.3899459100017,12.862135471942,12.3739558321145,11.9259291016308,11.5187571439109,11.1533878923628,10.831108184835,10.5536737740111,10.3234898140497,10.1438548040533,10.0192695222069,9.95576797111581,9.9610946025957,10.044201578343,10.2126988528679,10.465112663251,10.7719480200539,11.0387414797852,11.0632411390515,10.5918890511998,9.77641299060287,9.97445814770061,11.8667297335274,10.9083448008748,12.2034168139376,11.060418290774,11.4927594012936,12.3726039833423,13.4014041341765,13.3726191205335],[14.0965703502573,13.5663809586079,13.0750853334163,12.6229854961769,12.2105040063709,11.8382398040788,11.5070539440974,11.2182025674173,10.9735441291437,10.7758612403881,10.6293521845072,10.5403514185675,10.5182916804477,10.5767034202024,10.7333444362159,11.0066342577154,11.4011803366633,11.8682096304048,12.2289482639483,12.1213133650117,11.3068290662211,10.8149834549286,12.538507423087,12.5134260999897,12.7498084652201,12.1461486565549,12.8507839830097,13.0743815706143,14.9910326716331,14.9549586871955]],"color":"blue","alpha_stroke":1,"sizes":[10,100],"spans":[1,20],"type":"surface","inherit":true}},"layout":{"margin":{"b":40,"l":60,"t":25,"r":10},"scene":{"xaxis":{"title":[]},"yaxis":{"title":[]},"zaxis":{"title":[]}},"hovermode":"closest","showlegend":false},"source":"A","config":{"showSendToCloud":false},"data":[{"x":[-4,-3.72413793103448,-3.44827586206897,-3.17241379310345,-2.89655172413793,-2.62068965517241,-2.3448275862069,-2.06896551724138,-1.79310344827586,-1.51724137931034,-1.24137931034483,-0.96551724137931,-0.689655172413793,-0.413793103448276,-0.137931034482759,0.137931034482759,0.413793103448276,0.689655172413793,0.96551724137931,1.24137931034483,1.51724137931035,1.79310344827586,2.06896551724138,2.3448275862069,2.62068965517241,2.89655172413793,3.17241379310345,3.44827586206897,3.72413793103448,4],"y":[-4,-3.72413793103448,-3.44827586206897,-3.17241379310345,-2.89655172413793,-2.62068965517241,-2.3448275862069,-2.06896551724138,-1.79310344827586,-1.51724137931034,-1.24137931034483,-0.96551724137931,-0.689655172413793,-0.413793103448276,-0.137931034482759,0.137931034482759,0.413793103448276,0.689655172413793,0.96551724137931,1.24137931034483,1.51724137931035,1.79310344827586,2.06896551724138,2.3448275862069,2.62068965517241,2.89655172413793,3.17241379310345,3.44827586206897,3.72413793103448,4],"z":[[15.7417433483319,15.2051294365808,14.705283766665,14.241771601612,13.8140004216652,13.4211561412909,13.0621101177958,12.7352825992514,12.4384420699128,12.1684135925778,11.920668331915,11.6887888103448,11.4639002380353,11.2344570420108,10.9875799516382,10.715054934603,10.4308255619518,10.2106711450234,10.2532619437596,10.8752501597338,12.1101230061412,12.7078318169249,11.3672575023616,12.5266756483111,12.3978392858193,14.0972791632269,14.3681027131981,14.7063710622763,14.2110040656083,15.3307453710245],[14.9178298418846,14.3840529060493,13.8879327785247,13.4293073375634,13.0079357352367,12.6234536179562,12.2752999772785,11.9625968518469,11.6839510106344,11.4371281350077,11.218524065598,11.0223299920848,10.8392880343381,10.6550620720874,10.4488091946874,10.1943208600946,9.87077259944913,9.49937363138537,9.22840712062919,9.43466123887695,10.5224378188849,11.732359961777,10.7789815537373,10.9206497511802,11.8765405192537,12.8724471495259,12.9019041599986,13.8882442985131,12.6769859879092,13.7216149019558],[13.8782374942126,13.3476173683145,12.8556592814969,12.4025218378162,11.9883873342051,11.6134492730831,11.2778807627988,10.9817661727441,10.7249640060801,10.5068436098878,10.3257950561701,10.1783420003624,10.0575908022069,9.95066912182415,9.83495566440898,9.67402491659634,9.41846415276455,9.02861853894907,8.55838024673625,8.33285932039226,9.00719327455935,10.5380265214737,10.3338054479316,9.50601517157815,11.364590769354,11.4743384528593,11.3778943540983,12.8551025804806,11.4341910126712,12.2517893837105],[12.6378913128694,12.1098110290853,11.6212163713254,11.1725396027668,10.764332067521,10.3972877010383,10.072262357543,9.79027770006393,9.55248595596649,9.36004728799454,9.21382375508853,9.113703104057,9.05720166265094,9.03673081365999,9.0346038798909,9.01498786116519,8.91454529093677,8.64452508695893,8.14843561000593,7.60323046645783,7.72046695692153,9.18567682943478,9.87025769003905,8.40899289675646,10.7203340113264,10.0242699078922,9.9547520219823,11.6199566535928,10.5578457177423,11.0638680379462],[11.2938742673512,10.7669631448296,10.2799351871989,9.83336804012634,9.42801848030683,9.06487494681101,8.74522205998312,8.47071559199947,8.24345956420787,8.0660606699236,7.94159713398321,7.87335410636069,7.86399492710744,7.91346666830121,8.01426589544395,8.1417857742923,8.23755986166757,8.19007823069024,7.84922965624729,7.19129928736805,6.7731054126543,7.80563155318415,9.25490176613425,7.68413368721615,9.86396018580973,8.68156082428971,8.78380596189133,10.2783462813677,10.0370136768938,10.2393764570438],[9.99661734908962,9.46915775152429,8.98143483765151,8.53400021589278,8.12759151760803,7.76319836040277,7.44215291212753,7.16625369401145,6.93793214072249,6.76046792463336,6.63824195522113,6.5769618981228,6.58364774392537,6.66579796066577,6.82831904469103,7.06513254992613,7.33999331612797,7.55172379592247,7.50113956386339,6.97089483260019,6.19782978918821,6.55901327271268,8.42193421058404,7.29812981891621,8.80118930159828,7.59625404812156,7.96412733630847,8.97998826083662,9.77784090838005,9.7745734069102],[8.90525829336918,8.37569534763658,7.88522205348719,7.43420028906563,7.02312964396913,6.65270723951544,6.32391746244656,6.03816786133218,5.79749574167692,5.60488049291869,5.46470564862004,5.38340760686196,5.37027978543024,5.43814683399748,5.60286914821769,5.87870048467052,6.26237292971106,6.69306634545322,6.9816322513904,6.78159255961763,5.93952564248827,5.58993241084884,7.39271217001663,7.13969934553275,7.6215739895183,6.86458986819002,7.51312972403426,7.88434708583389,9.63140759391217,9.58156106011505],[8.14041652399345,7.60781953786221,7.11335663086319,6.65709256140444,6.23914062118043,5.85969856038565,5.51911086212039,5.21797640400999,4.95733378005065,4.73897794547356,4.56599421865892,4.44363769129031,4.38071733217425,4.39157818268905,4.49832787245925,4.73132247494145,5.12124109560971,5.6656697383289,6.24162678734347,6.47632125679516,5.87159847123694,4.98272162094124,6.26941554492811,7.05260966803163,6.47192423454154,6.50044614645578,7.36140153865146,7.11331292414904,9.43792090725641,9.51436919117438],[7.74839863256208,7.2127372830656,6.71422947163219,6.25262361949633,5.82761310047564,5.43883761011253,5.08589950048719,4.7684112578365,4.48610455579475,4.23905725190489,4.02814086305857,3.85586969529529,3.72795415533719,3.65600270119165,3.6618096278341,3.78282498294353,4.07457068681467,4.59375525256004,5.32005944419999,5.96500728715905,5.83354137681731,4.73690957787235,5.20472450553816,6.88204047379872,5.51274065258502,6.43121998119075,7.37330432347689,6.71502999608069,9.07413447164544,9.41227269620248],[7.68745662610326,7.14960995805766,6.6482022996951,6.18274039254918,5.75258868567678,5.35693574242704,4.99475978821011,4.66480191485404,4.36556650305233,4.0953912079777,3.85267511728942,3.63644561633435,3.44761995309818,3.29162512428974,3.18347450550856,3.15659859758743,3.27488568049136,3.63733819305912,4.33232913141433,5.24130446790056,5.67856642588778,4.76736084654861,4.35651075672414,6.52052757150049,4.87058499236444,6.51937304344629,7.387224258288,6.6496094949859,8.48992244418463,9.14748770917851],[7.8401768885896,7.30167245289424,6.79937055167422,6.33268262410214,5.90083279794021,5.50279930292466,5.13723945539171,4.80239654113655,4.49599148170395,4.21511504046355,3.95616895595909,3.71498232194167,3.48740702894665,3.27107811931414,3.06977274209525,2.90298115428514,2.82399877411794,2.94474241237796,3.43605209291495,4.38447733730667,5.31716482963173,4.92954032107305,3.84098818462826,5.93985351949499,4.60052219493744,6.60324688200591,7.26353200736939,6.80096682436,7.72315465341834,8.66309221287396],[8.04832335942074,7.51088390337891,7.00995873518043,6.54503865051079,6.11543770046997,5.7202269974936,5.35814139808797,5.02744769724828,4.72575968725528,4.44978457269081,4.19499454815246,3.95525815666227,3.72259290709264,3.48754370245686,3.24153057414638,2.98431968736196,2.74279787098801,2.60858433843345,2.78428836324492,3.53587893861248,4.74366920127094,5.06240486425145,3.69822570504768,5.19942131696513,4.66977954981919,6.54503601594075,6.92601854297473,7.01127038198912,6.8884585058301,7.98991798097136],[8.15979501151168,7.62482724347478,7.1271412305864,6.66645823440874,6.24238578873456,5.85436354152598,5.50157932820359,5.18283779170265,4.89635371616136,4.63942790370657,4.40794665777131,4.19563751643098,3.99305245790657,3.78645210131276,3.55744551633597,3.28614416822929,2.96504747252601,2.63828850688349,2.48012030553246,2.85700347623359,4.03793538629725,5.03619380210665,3.88025657790787,4.4286262158689,4.9674721888341,6.27168251847763,6.38451062081922,7.12777901264259,6.14320263136652,7.23740106858057],[8.07376272224031,7.54193986266262,7.04839418905645,6.59316014612519,6.17625521700086,5.79765387292242,5.45723794282219,5.15470464678801,4.88939953408211,4.66001800715983,4.46408124184803,4.29703716110194,4.15077596831005,4.01135124046518,3.85601870187175,3.65113715704826,3.3570444505633,2.95729077495215,2.54606231627296,2.48156196441109,3.34164477244736,4.7909361641491,4.26531281841691,3.78845692179402,5.33750882042146,5.79655638481541,5.73195463274617,7.0481688407982,5.64079682047769,6.56110602169904],[7.77059090634337,7.24165290566792,6.75192008537579,6.30172985164407,5.89150444872216,5.52176074693527,5.19310963668414,4.90623076692212,4.66179466412634,4.46027847804564,4.30157386249751,4.1841999808507,4.10379189132093,4.05033481151865,4.0034806174377,3.92581867472489,3.75729523398898,3.42575917938775,2.91737954625711,2.47580270032263,2.81625950778067,4.35424742023961,4.69396055999118,3.42372125234179,5.62491273494508,5.21548790771494,5.11683422037449,6.75089848021905,5.48516676997314,6.11656118673425],[7.31766449088828,6.79049520077645,6.30311626126976,5.85606791275906,5.45005185715623,5.08597397957534,4.76499294657995,4.48856914617995,4.25849909940665,4.0769001063188,3.946066451089,3.86802798444604,3.84345994148158,3.86925112577441,3.93348799002627,4.00609228594554,4.02443997216797,3.88208989083344,3.46129834635533,2.81885169374611,2.59510766223578,3.83313442810909,5.01641298828722,3.42007356882482,5.72181205128644,4.67832947665253,4.69910688448253,6.30160156653038,5.69991352359277,6.01310476246663],[6.84939204335754,6.32235044024808,5.83516788108375,5.38842712096793,4.9829010853213,4.61961584733168,4.29993400863404,4.02566331759646,3.7991931616626,3.6236526700034,3.50305791487615,3.44234706427301,3.44703515019191,3.52183782794651,3.66681154190866,3.86813632625735,4.07920189884444,4.19086409702575,4.01640762581782,3.40890714595882,2.74381734170696,3.38224052137369,5.13698045595917,3.77859634269856,5.59945132958808,4.34448600325758,4.60273565946099,5.83360959130024,6.22130972399858,6.28143804894887],[6.52731924174838,5.99872641021397,5.50952445393174,5.06016570672629,4.65126494737999,4.2836639439241,3.95852487304638,3.67746638355328,3.44276166457743,3.25762308698697,3.12659671815133,3.05606361304164,3.05474198250415,3.13377493524401,3.30517215710515,3.57547312056573,3.92791317701087,4.28303432875269,4.44055853916232,4.0934497299652,3.24084116310489,3.15795419422872,5.04246694615437,4.41547892581466,5.31672491650943,4.33561057983307,4.87890235655796,5.50838500072737,6.91721107944332,6.86451465683631],[6.49218924813086,5.96082660886793,5.46798874593392,5.01386333916198,4.59872458149051,4.22298013619902,3.88724687228676,3.59247399548156,3.34014391441135,3.132598951765,2.97356609789682,2.86897567930762,2.82816190554336,2.86538855590491,3.00105461306937,3.26011836104137,3.66063579243986,4.17641470089161,4.6530479889211,4.71450359439364,3.98385056653459,3.27199989951598,4.8060860678299,5.18688885130569,5.0026997027936,4.69950321144911,5.49081906151681,5.46760626872315,7.62627095657998,7.63446935765987],[6.82217635068928,6.28764728106601,5.79063597854804,5.33100977388697,4.90862090581625,4.52332195639696,4.17500142745009,3.8636573992637,3.58954158651524,3.35343116573274,3.1571280440313,3.00435196030652,2.90228314143213,2.86407317722573,2.91245682957048,3.08340467698853,3.4244351317329,3.97039446944641,4.65856774265406,5.15551662297161,4.82010241498731,3.75828316274925,4.56573448833592,5.93165496084252,4.81831376723637,5.39592588437809,6.32464516367225,5.7911239796841,8.20582093923759,8.43056283953667],[7.5096872856803,6.97253481982641,6.47205103744102,6.00782229057096,5.5793227127663,5.1858935520068,4.82672803067304,4.50087367329913,4.20727680280668,3.94491881787471,3.71314181588534,3.51235109381772,3.34544212147963,3.22055148787186,3.1560030602918,3.18810523185103,3.37973746719014,3.81644368750667,4.54581212343624,5.37594795790848,5.59176641517007,4.56282891876561,4.48307028944288,6.51909589017047,4.90861705886719,6.30854343242294,7.22327022415744,6.47326469972759,8.57420851681427,9.10688203974301],[8.46361458293769,7.92516026318482,7.42293543939448,6.95636823736237,6.52471073940528,6.12698834794326,5.76193842190199,5.42794058273664,5.12294844645951,4.84444990981061,4.58952249689557,4.35513678852389,4.1390442498074,3.94195188847178,3.77233496224864,3.65606495045221,3.65272252056773,3.87316357953016,4.46115744982518,5.42329873697381,6.18276218176811,5.55979892855245,4.6955399844213,6.88780052961405,5.359726974727,7.27944667909389,8.03293741914796,7.42451460143716,8.73502794520049,9.57564070098241],[9.53637324144289,8.99832494289478,8.49660718039962,8.03065956052277,7.59973406330225,7.20282946314042,6.8386019516447,6.50524410401827,6.20032410580226,5.92058183315846,5.66169749876047,5.41810596514229,5.18308195862203,4.94969309696532,4.71405094594956,4.48391255802414,4.29787889135952,4.25922069874032,4.56522400571007,5.419015852555,6.55320677190216,6.58883501396084,5.27559045992955,7.06385213068094,6.17327251773437,8.15601291092183,8.64887177819299,8.49808321660994,8.77605363028477,9.83323045757803],[10.5676974430779,10.0316425516669,9.53252947975687,9.06997561471787,8.64345481361554,8.25223634239686,7.8952940083648,7.57116970436877,7.27776784066738,7.01204773369425,6.76957420688971,6.54389789166144,6.32581253990229,6.10280360446274,5.85977287480388,5.58406102574731,5.28183424564861,5.01813193218979,4.98521102999322,5.52245187478394,6.75125018210199,7.50264628660985,6.20916993793158,7.15308794835526,7.26591088052513,8.83619756189682,9.04637277057326,9.53346436774132,8.84319255873602,9.96129163912876],[11.4322003731495,10.8991347195605,10.4039510103335,9.94655795618282,9.52680742720013,9.14445603048862,8.79909971774216,8.49006240129635,8.2162065946611,7.97561339520557,7.76504840176658,7.57909190254212,7.40878868306503,7.23975579645995,7.05016573830953,6.81069953472528,6.49325875172565,6.10535060956203,5.77714959718539,5.88357624270354,6.89878655472539,8.21071858199106,7.39989792315852,7.30969141558051,8.49405991184324,9.29881653631073,9.28814433251237,10.4040673166052,9.09718554484038,10.1024860243699],[12.0765834922904,11.546615833371,11.0555200975265,10.6035234094463,10.1908996106661,9.81796487771363,9.4850572103757,9.19248317344973,8.9404009394774,8.72858283236198,8.5559550538323,8.41973588010801,8.31387864272365,8.22640021883302,8.13522124996354,8.00306369079788,7.77590285993017,7.40148598858678,6.90927506775863,6.59847095456066,7.1552813770356,8.70617506810896,8.69766667977311,7.69043730617271,9.69650696117394,9.60983271327629,9.50568710672011,11.0547833075179,9.66590645208122,10.4181611224781],[12.5354531902933,12.0077729348214,11.5197106530065,11.0717444829301,10.6644889734923,10.2987260434079,9.9754354371398,9.69581539704741,9.4612728160757,9.27333880887767,9.13341883600256,9.04219465452159,8.99832252626559,8.99577521433122,9.01877368518038,9.03314330042696,8.97496527197858,8.74773315731916,8.27145343103522,7.68081767188192,7.67044916354755,9.06781698566698,9.94318658146074,8.40816790214701,10.7422496773291,9.9027810758738,9.86027485620587,11.5183394134694,10.6064148749375,11.040469354388],[12.9210511371807,12.3941688935595,11.9071853511646,11.4606877711931,11.0554490389222,10.6924847762943,10.3731257275386,10.0991062271356,9.87266438470946,9.69663573841868,9.57448792898033,9.51016387891194,9.50742154251527,9.5679781866945,9.68703762349782,9.84367014564063,9.98303113321069,9.99299932219743,9.70786421813153,9.05783760625475,8.5398247966681,9.43673891819225,11.0152562413163,9.49830638538668,11.569371723705,10.3391414225405,10.4950955302026,11.9055865854658,11.8880233699097,12.0331510475986],[13.3899459100017,12.862135471942,12.3739558321145,11.9259291016308,11.5187571439109,11.1533878923628,10.831108184835,10.5536737740111,10.3234898140497,10.1438548040533,10.0192695222069,9.95576797111581,9.9610946025957,10.044201578343,10.2126988528679,10.465112663251,10.7719480200539,11.0387414797852,11.0632411390515,10.5918890511998,9.77641299060287,9.97445814770061,11.8667297335274,10.9083448008748,12.2034168139376,11.060418290774,11.4927594012936,12.3726039833423,13.4014041341765,13.3726191205335],[14.0965703502573,13.5663809586079,13.0750853334163,12.6229854961769,12.2105040063709,11.8382398040788,11.5070539440974,11.2182025674173,10.9735441291437,10.7758612403881,10.6293521845072,10.5403514185675,10.5182916804477,10.5767034202024,10.7333444362159,11.0066342577154,11.4011803366633,11.8682096304048,12.2289482639483,12.1213133650117,11.3068290662211,10.8149834549286,12.538507423087,12.5134260999897,12.7498084652201,12.1461486565549,12.8507839830097,13.0743815706143,14.9910326716331,14.9549586871955]],"type":"surface","name":"blue","frame":null}],"highlight":{"on":"plotly_click","persistent":false,"dynamic":false,"selectize":false,"opacityDim":0.2,"selected":{"opacity":1},"debounce":0},"shinyEvents":["plotly_hover","plotly_click","plotly_selected","plotly_relayout","plotly_brushed","plotly_brushing","plotly_clickannotation","plotly_doubleclick","plotly_deselect","plotly_afterplot","plotly_sunburstclick"],"base_url":"https://plot.ly"},"evals":[],"jsHooks":[]}</script>
<p>
Again, we’ll apply gradient descent from a set of initial guess positions:
</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="co">#define a set of initial guess values</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2">x0 =<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.5</span>,<span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb96-3" data-line-number="3">x1 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.1</span>,<span class="op">-</span><span class="fl">1.3</span>)</a>
<a class="sourceLine" id="cb96-4" data-line-number="4">x2 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.5</span>,<span class="fl">1.3</span>)</a>
<a class="sourceLine" id="cb96-5" data-line-number="5">x3 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">1.2</span>,<span class="op">-</span><span class="fl">1.4</span>)</a>
<a class="sourceLine" id="cb96-6" data-line-number="6">x4 =<span class="st"> </span><span class="kw">c</span>(<span class="op">-</span><span class="fl">0.5</span>,<span class="op">-</span><span class="fl">0.5</span>)</a>
<a class="sourceLine" id="cb96-7" data-line-number="7"><span class="co">#run the gradient descent algorithm from each</span></a>
<a class="sourceLine" id="cb96-8" data-line-number="8">gd0 =<span class="st"> </span><span class="kw">grad.descent</span>(Example_<span class="dv">7</span>_function,x0,<span class="dt">step.size=</span><span class="fl">0.01</span>,<span class="dt">max.iter=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb96-9" data-line-number="9">gd1 =<span class="st"> </span><span class="kw">grad.descent</span>(Example_<span class="dv">7</span>_function,x1,<span class="dt">step.size=</span><span class="fl">0.01</span>,<span class="dt">max.iter=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb96-10" data-line-number="10">gd2 =<span class="st"> </span><span class="kw">grad.descent</span>(Example_<span class="dv">7</span>_function,x2,<span class="dt">step.size=</span><span class="fl">0.01</span>,<span class="dt">max.iter=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb96-11" data-line-number="11">gd3 =<span class="st"> </span><span class="kw">grad.descent</span>(Example_<span class="dv">7</span>_function,x3,<span class="dt">step.size=</span><span class="fl">0.01</span>,<span class="dt">max.iter=</span><span class="dv">1000</span>)</a>
<a class="sourceLine" id="cb96-12" data-line-number="12">gd4 =<span class="st"> </span><span class="kw">grad.descent</span>(Example_<span class="dv">7</span>_function,x4,<span class="dt">step.size=</span><span class="fl">0.01</span>,<span class="dt">max.iter=</span><span class="dv">1000</span>)</a></code></pre></div>
<p>
From some initial guess values, the algorithm successfully reaches the global minimum. From others, it gets stuck at a local minimum. The plot below shows the paths followed by the algorithm, leading to two separate valley bottoms.
</p>
<p><img src="_main_files/figure-html/gradient-descent-plot-paths-complicated-function-1.png" title="Surface plot with multiple minima and search paths" alt="Surface plot with multiple minima and search paths" width="672" /></p>
<p>
The table below shows the final estimate and the corresponding objective value reached by gradient descent starting from those five starting points.
</p>
<pre><code>               x1         x2 objective value
x0_out  1.0697802 -0.5784429        2.810101
x1_out  1.0633307 -0.6030409        2.810100
x2_out -0.3775909  1.1636290        2.426837
x3_out -0.3776268  1.1636047        2.426838
x4_out -0.3776816  1.1635677        2.426839</code></pre>
</div>
<div id="exercise-1" class="section level4">
<h4><span class="header-section-number">8.4.1.3</span> Exercise 1</h4>
<p>Consider the following dataset, which corresponds to measurements of drug concentration in the blood over time. try fitting the data to an exponential <span class="math inline">\(C(t) = a e^{-r t}\)</span>. You’ll find that the best-fit model is not satisfactory. Next, try fitting a biexponential <span class="math inline">\(C(t) = a_1 e^{-r_1 t} + a_2 e^{-r_2 t}\)</span>. You’ll find a more suitable agreement. For fitting, you can either use the <code>nls</code> command or the gradient descent function above (along with a sum of squared errors function). For <code>nls</code>, you may find the algorithm is very sensitive to your choice of initial guess (and will fail if the initial guess is not fairly accurate). For gradient descent, you’ll need to use a small stepsize, e.g. <span class="math inline">\(10^{-5}\)</span>, and a large number of iterations.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1">t &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>, <span class="dv">10</span>, <span class="dv">20</span>, <span class="dv">30</span>, <span class="dv">40</span>, <span class="dv">50</span>, <span class="dv">60</span>, <span class="dv">70</span>, <span class="dv">80</span>, <span class="dv">90</span>, <span class="dv">100</span>)</a>
<a class="sourceLine" id="cb98-2" data-line-number="2">C &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">5.5850</span>, <span class="fl">4.392</span>, <span class="fl">3.2564</span>, <span class="fl">2.9971</span>, <span class="fl">2.6493</span>, <span class="fl">2.3863</span>, <span class="fl">2.0838</span>, <span class="fl">2.0862</span>, <span class="fl">1.7009</span>, <span class="fl">1.7339</span>, <span class="fl">1.4162</span>)</a>
<a class="sourceLine" id="cb98-3" data-line-number="3">BE_data=<span class="kw">cbind</span>(t,C)</a>
<a class="sourceLine" id="cb98-4" data-line-number="4"><span class="kw">plot</span>(BE_data)</a></code></pre></div>
<p><img src="_main_files/figure-html/exercise-1-dataset-1.png" title="Drug concentration data for exercise 1" alt="Drug concentration data for exercise 1" width="672" /></p>
<!--SOLUTION #true values: 3,3, 0.1, 0.01-->
<!-- ```{r exercise-1-grad-descent-soln, echo = TRUE} -->
<!-- #define SSE -->
<!-- determine_sse <- function(x){ -->
<!--   pred<-(x[1]*exp(-x[2]*T)+x[3]*exp(-x[4]*T)) -->
<!--   obs<-C -->
<!--   sse<-sum((obs - pred)^2) -->
<!--   return(sse) -->
<!-- } -->
<!-- x0 = c(2,0.1,4,0.01) -->
<!-- gd = grad.descent(determine_sse,x0,step.size=0.00001,max.iter=20000,  -->
<!--                         stopping.deriv=0.01) -->
<!-- x0_out <- c(gd$x, simpleFun(gd$x)) -->
<!-- x0_out -->
<!-- plot(BE_data) -->
<!-- curve(x0_out[1]*exp(-x0_out[2]*x)+x0_out[3]*exp(-x0_out[4]*x), from = 0, to = 100, add=TRUE, col='firebrick') -->
<!-- ``` -->
<!-- ```{r exercise-1-grad-descent-part-2-soln, echo = TRUE} -->
<!-- #define SSE -->
<!-- determine_sse <- function(x){ -->
<!--   pred<-(x[1]*exp(-x[2]*T)) -->
<!--   obs<-C -->
<!--   sse<-sum((obs - pred)^2) -->
<!--   return(sse) -->
<!-- } -->
<!-- x0 = c(3,0.01) -->
<!-- gd = grad.descent(determine_sse,x0,step.size=0.00001,max.iter=20000,  -->
<!--                         stopping.deriv=0.01) -->
<!-- x0_out <- c(gd$x, simpleFun(gd$x)) -->
<!-- x0_out -->
<!-- plot(BE_data) -->
<!-- curve(x0_out[1]*exp(-x0_out[2]*x), from = 0, to = 100, add=TRUE, col='firebrick') -->
<!-- ``` -->
<!-- ```{r exercise_1_nls_soln, echo = FALSE} -->
<!-- BEmodel.nls <- nls(C ~ a_1*exp(r_1*T) + a_2*exp(r_2*T),start = list(a_1 = 3, a_2=3, r_1 = 0.01, r_2=0.1)) -->
<!-- summary(BEmodel.nls) -->
<!-- params <- summary(BEmodel.nls)$coeff[,1] #extracting the parameter estimates -->
<!-- plot(BE_data) -->
<!-- curve(params[1]*exp(params[3]*x) + params[2]*exp(params[4]*x), from = 0, to = 100, add=TRUE, col='firebrick') -->
<!-- ``` -->
<!-- ```{r exercise_1_part_2_nls_soln, echo = FALSE} -->
<!-- BEmodel.nls <- nls(C ~ a*exp(r*T),start = list(a = 3,  r = 0.01)) -->
<!-- summary(BEmodel.nls) -->
<!-- params <- summary(BEmodel.nls)$coeff[,1] #extracting the parameter estimates -->
<!-- plot(BE_data) -->
<!-- curve(params[1]*exp(params[2]*x), from = 0, to = 100, add=TRUE, col='firebrick') -->
<!-- ``` -->
</div>
</div>
<div id="global-optimization" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Global Optimization</h3>
<p>
The default optimization algorithm used by <code>nls</code> is the Gauss-Newton method, which is a generalization of Newton’s method. (You may recall using Newton’s method to solve nonlinear equations in an introductory calculus course.) Gauss-Newton is a refinement of gradient descent in which the local curvature of the function is used to predict the position of the bottom of the local valley.
</p>
<p>
Both gradient descent and the Gauss-Newton method are designed to reach the bottom of the valley in which the initial guess lies. If that’s not the global minimum, then the algorithm will not be successful. So, how does one select a ‘good’ initial guess? Unfortunately, there’s no general answer to this question. In some cases, one can use previous knowledge of the system to identify a solid initial guess. If no such previous knowledge is available, sometimes a ‘wild guess’ is all we have. In those cases, we may have little confidence that the algorithm will arrive at a global minimum. The simplest way to improve the chance of achieving a global minimum is the <strong>multi-start</strong> strategy: choose many initial guesses, and run the algorithm from each (as in Example 7 above). This can be computationally expensive, but if the initial guesses are spread widely over the parameters space, one can expect that the global minimum will be achieved.
</p>
<p>
A number of methods have been developed to complement the multi-start approach. These are known as <strong>global optimization routines</strong>. They are also known as <em>heuristic</em> methods, because their performance cannot be guaranteed in general: there are no guarantees that they’ll find the global minimum, nor are there solid estimates of how many iterations will be required for them to carry out a satisfactory search of the parameter space. We will consider two commonly used heuristic methods: simulated annealing and genetic algorithms.
</p>
<div id="simulated-annealing" class="section level4">
<h4><span class="header-section-number">8.4.2.1</span> Simulated Annealing</h4>
<p>
<strong>Simulated annealing</strong> is motivated by the process of annealing, which is a heat treatment by which the physical properties of metals can be altered. Simulated annealing is an iterative algorithm; the algorithm starts at an initial guess, and then steps through the search space. In contrast to gradient descent, the steps taken in simulated annealing are partially random. Consequently, the path followed from a particular initial condition won’t be repeated if the algorithm is run again from the same point. (Algorithms that incorporate randomness are often referred to as <em>Monte Carlo</em> methods, in reference to the European gambling centre.) At each step, the algorithm begins by identifying a candidate next position. (This point could be selected by a variant of gradient descent, or some other method). The value of the objective at this candidate point is then compared with the objective value at the current point. If the objective is lower at the candidate point (i.e. this step takes the algorithm downhill), then the step is taken and a new iteration begins. If the value of the objective is larger at the candidate point (i.e. the step makes things <em>worse</em>), the step can still be taken, but only with a small probability. Both the size of the steps and the probability of accepting ‘wrong’ (uphill) steps are tied to a <strong>cooling schedule</strong>: a decreasing ‘temperature’ profile. At high temperatures, large steps are considered and ‘wrong’ steps are taken frequently; as the temperature drops, only smaller steps are considered, and fewer ‘wrong’ steps are allowed. By analogy, imagine a ping-pong ball resting on a curved landscape. One strategy to move the ball to the lowest valley bottom is to shake the table. To following a strategy inspired by simulated annealing, we could begin by applying violent shakes (high temperature) which would result in the ball bouncing across much of the landscape. By slowly reducing the severity of the shaking, we could ensure the ball would settle into a local minimum at a valley bottom. The hope is that if the cooling schedule is well-chosen, the ball would have sampled many valleys, and would end up at the bottom of the lowest valley. Simulated annealing can be combined with a multi-start strategy to further ensure broad sampling of the search space.
</p>
</div>
<div id="example-8" class="section level4">
<h4><span class="header-section-number">8.4.2.2</span> Example 8</h4>
<p>
<p>To illustrate, we’ll apply simulated annealing to the optimization task in Example 7 above, using the same initial guess points. We’ll use the <code>GenSA</code> library to implement the algorithm (described in detail <a href="https://cran.r-project.org/web/packages/GenSA/GenSA.pdf">here</a>). Calls to <code>GenSA</code> require that we specify (i) the objective function; (ii) an initial guess; and (iii) upper and lower bounds for the search values for each parameter. (Optional input parameters allow the user to modify internal features of the algorithm such as the cooling schedule and stepping protocol.) To begin, we apply the algorithm at the first initial guess:</p>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1"><span class="kw">library</span>(GenSA)</a>
<a class="sourceLine" id="cb99-2" data-line-number="2"></a>
<a class="sourceLine" id="cb99-3" data-line-number="3">out0 &lt;-<span class="st"> </span><span class="kw">GenSA</span>(<span class="dt">par =</span> x0, <span class="dt">lower =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>,<span class="op">-</span><span class="dv">2</span>), <span class="dt">upper =</span> <span class="kw">c</span>(<span class="dv">2</span>,<span class="dv">2</span>),<span class="dt">fn =</span> Example_<span class="dv">7</span>_function)</a>
<a class="sourceLine" id="cb99-4" data-line-number="4">out0[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>,<span class="st">&quot;par&quot;</span>)]</a></code></pre></div>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<p>
The result of this function call is recorded in the variable <code>out0</code> which indicates the minimal value of the objective achieved (2.426731) and the parameter values at which this minimum occurs <span class="math inline">\((x,y)= (-0.3629442, 1.1734789)\)</span> (matching the solution founds above). Next, we’ll call <code>GenSA</code> starting from each of the initial guesses that were provided to the gradient descent algorithm in Example 7.
</p>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<pre><code>$value
[1] 2.426731

$par
[1] -0.3629442  1.1734789</code></pre>
<p>
We see that in each case, simulated annealing has avoided getting ‘stuck’ in the local minima. It has achieved the same optimized value from every initial guess.
</p>
<p>
The plot below provides some insight into how the simulated annealing runs proceed. Iterations (steps) are shown along the horizontal axis. The vertical axis shows values of the objective function. The blue points represents the function value at the current position, while the red shows the minimum achieved so far. The minimum is found rather quickly, but the algorithm continues to explore the search space.
</p>
<p><img src="_main_files/figure-html/output-simulated-annealing-complicated-function,%20echo-TRUE-1.png" width="672" /></p>
</div>
<div id="example-9" class="section level4">
<h4><span class="header-section-number">8.4.2.3</span> Example 9</h4>
<p>
To give the simulated annealing algorithm a more challenging task, consider the following function, which has many local minima. (Near the origin, the graph resembles an egg carton).
<span class="math display">\[f(x,y) = (y+47)\sin\left (\sqrt{|y+(x/2)+47|)}-x\sin(\sqrt{|x-(y+47)|} + \frac{1}{1000}\left(x^2+y^2\right)\right)\]</span>
</p>
<p><img src="_main_files/figure-html/plot-wireframe-egg-carton-function-1.png" title="Surface plot with many minima, resembling an egg carton" alt="Surface plot with many minima, resembling an egg carton" width="672" /></p>
<p>We will use simulated annealing to search for the minimum value. We’ll start the algorithm from two different initial guesses. Both runs result in the same solution:</p>
<div class="sourceCode" id="cb106"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb106-1" data-line-number="1"><span class="co">#define a set of initial guess values</span></a>
<a class="sourceLine" id="cb106-2" data-line-number="2"></a>
<a class="sourceLine" id="cb106-3" data-line-number="3"><span class="co">#initial guesses</span></a>
<a class="sourceLine" id="cb106-4" data-line-number="4">x1 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">100</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb106-5" data-line-number="5">x2 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">100</span>)</a>
<a class="sourceLine" id="cb106-6" data-line-number="6"></a>
<a class="sourceLine" id="cb106-7" data-line-number="7"><span class="co">#specify bounds</span></a>
<a class="sourceLine" id="cb106-8" data-line-number="8">lower &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="op">-</span><span class="dv">200</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb106-9" data-line-number="9">upper &lt;-<span class="st"> </span><span class="kw">rep</span>(<span class="dv">200</span>, <span class="dv">2</span>)</a>
<a class="sourceLine" id="cb106-10" data-line-number="10"></a>
<a class="sourceLine" id="cb106-11" data-line-number="11"></a>
<a class="sourceLine" id="cb106-12" data-line-number="12">egg.out1 &lt;-<span class="st"> </span><span class="kw">GenSA</span>(<span class="dt">par =</span> x1, <span class="dt">lower =</span> lower, <span class="dt">upper =</span> upper,<span class="dt">fn =</span> f.egg)</a>
<a class="sourceLine" id="cb106-13" data-line-number="13">egg.out2 &lt;-<span class="st"> </span><span class="kw">GenSA</span>(<span class="dt">par =</span> x2, <span class="dt">lower =</span> lower, <span class="dt">upper =</span> upper,<span class="dt">fn =</span> f.egg)</a>
<a class="sourceLine" id="cb106-14" data-line-number="14">egg.out1[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>,<span class="st">&quot;par&quot;</span>)]</a></code></pre></div>
<pre><code>$value
[1] -266.8175

$par
[1] -161.38281   95.54245</code></pre>
<div class="sourceCode" id="cb108"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb108-1" data-line-number="1">egg.out2[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>,<span class="st">&quot;par&quot;</span>)]</a></code></pre></div>
<pre><code>$value
[1] -266.8175

$par
[1] -161.38281   95.54245</code></pre>
<p>From the plots below we see that the algorithm occasionally gets stuck at local minima, but in both cases has visited the lowest point.</p>
<p><img src="_main_files/figure-html/plot-egg-carton-simulated-annealing-traces-1.png" title="Progress of simulated annealing algorithm" alt="Progress of simulated annealing algorithm" width="672" /><img src="_main_files/figure-html/plot-egg-carton-simulated-annealing-traces-2.png" title="Progress of simulated annealing algorithm" alt="Progress of simulated annealing algorithm" width="672" /></p>
<p>We next consider a heuristic algorithm in which multiple paths through the search space are followed simultaneously.</p>
</div>
<div id="genetic-algorithms" class="section level4">
<h4><span class="header-section-number">8.4.2.4</span> Genetic Algorithms</h4>
<p>
<strong>Genetic algorithms</strong> are inspired by Darwinian evolution. The algorithm begins with the specification of a population of initial guesses. At each iteration of the algorithm, this population ‘evolves’ toward improved estimates of the global minimum. This ‘evolution’ step involves three substeps: selection, mutation, and cross-over. In the selection step, the population is pruned by removing a fraction that are not sufficiently fit. (Here fitness corresponds to the value of the objective function being minimized.) Then, mutations are introduced into the remaining population by adding small random perturbations to their position in the search space. Finally, a new generation is produced by crossing members of the current population. This can be done in several ways; the simplest is to generate crosses as averages of the numerical values of the two ‘parents’. Through several generations, this process leads to a population with high fitness (minimal objective) after a thorough exploration of the search space. Genetic algorithms are a subset of the more general class of <em>evolutionary algorithms</em> all of which involve simultaneous exploration of the search space through multiple paths.
</p>
</div>
<div id="example-10" class="section level4">
<h4><span class="header-section-number">8.4.2.5</span> Example 10</h4>
</p>
To implement a genetic algorithm, we’ll make use of the <code>ga</code> function, from <code>Library(GA)</code>, described <a href="https://cran.r-project.org/web/packages/GA/GA.pdf">here</a>. The call to <code>ga</code> requires that we specify the objective function, lower and upper bounds to define the search space, and the number of iterations to execute. The initial population is generated automatically. The <code>ga</code> routine maximizes the objective function, so we enter our objective with a negative sign to achieve minimization. We consdier again the function from Example 7.
</p>
<div class="sourceCode" id="cb110"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb110-1" data-line-number="1">Example_<span class="dv">7</span>_fun =<span class="st"> </span><span class="cf">function</span>(x,y) {</a>
<a class="sourceLine" id="cb110-2" data-line-number="2">  <span class="kw">return</span>((<span class="dv">1</span><span class="op">/</span><span class="dv">2</span><span class="op">*</span>x<span class="op">^</span><span class="dv">2</span><span class="op">+</span><span class="dv">1</span><span class="op">/</span><span class="dv">4</span><span class="op">*</span>y<span class="op">^</span><span class="dv">2</span><span class="op">+</span><span class="dv">3</span>)<span class="op">+</span><span class="kw">cos</span>(<span class="dv">2</span><span class="op">*</span>x<span class="op">+</span><span class="dv">1</span><span class="op">-</span><span class="kw">exp</span>(y)))</a>
<a class="sourceLine" id="cb110-3" data-line-number="3">}</a>
<a class="sourceLine" id="cb110-4" data-line-number="4"></a>
<a class="sourceLine" id="cb110-5" data-line-number="5"><span class="kw">library</span>(GA)</a>
<a class="sourceLine" id="cb110-6" data-line-number="6">ga &lt;-<span class="st"> </span><span class="kw">ga</span>(<span class="dt">type =</span> <span class="st">&quot;real-valued&quot;</span>, <span class="dt">fitness =</span> <span class="cf">function</span>(x) <span class="op">-</span><span class="kw">Example_7_fun</span>(x[<span class="dv">1</span>],x[<span class="dv">2</span>]),</a>
<a class="sourceLine" id="cb110-7" data-line-number="7">         <span class="dt">lower =</span> <span class="kw">c</span>(<span class="op">-</span><span class="dv">2</span>, <span class="dv">-2</span>), <span class="dt">upper =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">2</span>), <span class="dt">maxiter =</span> <span class="dv">30</span>)</a></code></pre></div>
<pre><code>GA | iter = 1 | Mean = -4.041041 | Best = -2.873942
GA | iter = 2 | Mean = -3.552414 | Best = -2.480774
GA | iter = 3 | Mean = -3.385857 | Best = -2.480774
GA | iter = 4 | Mean = -3.316562 | Best = -2.475568
GA | iter = 5 | Mean = -3.233971 | Best = -2.453920
GA | iter = 6 | Mean = -2.942318 | Best = -2.429493
GA | iter = 7 | Mean = -3.057510 | Best = -2.429493
GA | iter = 8 | Mean = -2.929332 | Best = -2.427140
GA | iter = 9 | Mean = -2.862539 | Best = -2.427125
GA | iter = 10 | Mean = -2.611589 | Best = -2.427125
GA | iter = 11 | Mean = -2.665873 | Best = -2.426933
GA | iter = 12 | Mean = -2.649276 | Best = -2.426795
GA | iter = 13 | Mean = -2.574107 | Best = -2.426795
GA | iter = 14 | Mean = -2.667271 | Best = -2.426775
GA | iter = 15 | Mean = -2.516660 | Best = -2.426756
GA | iter = 16 | Mean = -2.705621 | Best = -2.426756
GA | iter = 17 | Mean = -2.598244 | Best = -2.426756
GA | iter = 18 | Mean = -2.580032 | Best = -2.426756
GA | iter = 19 | Mean = -2.584246 | Best = -2.426745
GA | iter = 20 | Mean = -2.654025 | Best = -2.426745
GA | iter = 21 | Mean = -2.551965 | Best = -2.426745
GA | iter = 22 | Mean = -2.612507 | Best = -2.426745
GA | iter = 23 | Mean = -2.675908 | Best = -2.426745
GA | iter = 24 | Mean = -2.628224 | Best = -2.426745
GA | iter = 25 | Mean = -2.664589 | Best = -2.426745
GA | iter = 26 | Mean = -2.631207 | Best = -2.426745
GA | iter = 27 | Mean = -2.598906 | Best = -2.426745
GA | iter = 28 | Mean = -2.553829 | Best = -2.426745
GA | iter = 29 | Mean = -2.591655 | Best = -2.426745
GA | iter = 30 | Mean = -2.482629 | Best = -2.426745</code></pre>
<p>Plotting the results of the genetic algorithm search, we see improvement in the overall behaviour from generation to generation.</p>
<div class="sourceCode" id="cb112"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb112-1" data-line-number="1"><span class="kw">plot</span>(ga)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-GA-output-1.png" width="672" /></p>
<p>The summary displayed below shows that the solutions reached by the genetic algorithm agrees with the solution found by simulated annealing.</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="kw">summary</span>(ga)</a></code></pre></div>
<pre><code>-- Genetic Algorithm ------------------- 

GA settings: 
Type                  =  real-valued 
Population size       =  50 
Number of generations =  30 
Elitism               =  2 
Crossover probability =  0.8 
Mutation probability  =  0.1 
Search domain = 
      x1 x2
lower -2 -2
upper  2  2

GA results: 
Iterations             = 30 
Fitness function value = -2.426745 
Solution = 
             x1       x2
[1,] -0.3675713 1.169812</code></pre>
</div>
<div id="exercise-2" class="section level4">
<h4><span class="header-section-number">8.4.2.6</span> Exercise 2</h4>
<p>Apply the genetic algorithm with <code>ga</code> to confirm the minimum of the egg-carton function in Example 9.</p>
<!-- SOLUTION -->
<!-- Here is the code for the egg-carton function: -->
<!-- ```{r ex2-define egg-carton, echo = TRUE} -->
<!-- f.egg<- function(x,y){ -->
<!--   -(y+47)*sin(sqrt(abs(y+(x/2)+47)))-x*sin(sqrt(abs(x-(y+47))))+ 0.001*x^2 + 0.001*y^2 -->
<!-- } -->
<!-- ``` -->
<!-- Here is the code for the genetic algorithm: -->
<!-- ```{r ex2-call-GA-egg-carton, echo = TRUE} -->
<!-- ga1 <- ga(type = "real-valued", fitness = function(x) -f.egg(x[1],x[2]), -->
<!--          lower = c(-200, -200), upper = c(200, 200), maxiter = 30) -->
<!-- ``` -->
<!-- Now try plotting the results of the genetic algorithm search as well as getting a summary of the results to confirm that the fitness function value obtained through this method is the same as the value obtained through simulated annealing.  -->
<!-- ```{r ex2-results, echo = TRUE} -->
<!-- plot(ga1) -->
<!-- summary(ga1) -->
<!-- ``` -->
</div>
</div>
</div>
<div id="calibration-of-dynamic-models" class="section level2">
<h2><span class="header-section-number">8.5</span> Calibration of Dynamic Models</h2>
<p>
The principles of nonlinear regression carry over directly to calibration of more complex models. In many domains of biology, dynamic models are used to describe the time-varying behaviour of systems (from, e.g., biomolecular networks to physiology to ecology). Dynamic models take many forms; a commonly used formulation is based on ordinary differential equations (i.e. <em>rate equations</em>). These models are deterministic (i.e., they do not incorporate random effects) and assume that the dynamics occur in a spatially homogeneous (well-mixed) environment. Despite these limitations, these models can describe a wide variety of dynamic behaviours, and are useful for investigations across a range of biological domains.
</p>
<p>
Differential equation models used in biology often take the form
<span class="math display">\[\begin{equation*}
    \frac{d}{dt} {\bf x}(t) = {\bf f}({\bf x}(t), {\bf p})
\end{equation*}\]</span>
where components of the time-varying vector <span class="math inline">\({\bf x}(t)\)</span> are the <em>states</em> of the system (e.g. population sizes, molecular concentrations), the components of vector <span class="math inline">\({\bf p}\)</span> are the <em>model parameters</em>: numerical values that represent fixed features of the system and its environment (e.g. interaction strengths, temperature, nutrient availability), and the vector-valued function <span class="math inline">\({\bf f}\)</span> describes the rate of change of the state variables. As a concrete example, consider the Lotka-Volterra equations, a classical model describing interacting predator and prey populations:
<span class="math display">\[\begin{eqnarray*}
    \frac{d}{dt} x_1(t) &amp;=&amp; \alpha x_1(t) - \beta x_1(t) x_2(t)\\
    \frac{d}{dt} x_2(t) &amp;=&amp; \gamma x_1(t) x_2(t) - \delta x_2(t)\\
\end{eqnarray*}\]</span>
</p>
Here <span class="math inline">\(x_1\)</span> is the size of the prey population; <span class="math inline">\(x_2\)</span> is the size of the predator population. The prey are presumed to have access to resources that support exponential growth in the absence of predation: growth at rate <span class="math inline">\(\alpha x_1(t)\)</span>. Interactions between prey and predators, assumed to occur at rate <span class="math inline">\(x_1(t) x_2(t)\)</span>, lead to decrease of the prey population and increase of the predator population (characterized by parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> respectively). Finally, the prey suffer an exponential decline in population size in the absence of prey: decay at rate <span class="math inline">\(\delta x_2(t)\)</span>. A simulation of the model, shown below, demonstrates the a boom-bust cycle of oscillations in both populations. Simulation of the model requires specification of numerical values for each of the four model parameters, <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, <span class="math inline">\(\delta\)</span>, and the two initial population sizes <span class="math inline">\(x_1(0)\)</span> and <span class="math inline">\(x_2(0)\)</span>. We generate the simulation by a call to the <code>ode</code> function from the deSolve library described <a href="https://cran.r-project.org/web/packages/deSolve/vignettes/deSolve.pdf">here</a>.
</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1"><span class="kw">library</span>(deSolve)</a>
<a class="sourceLine" id="cb115-2" data-line-number="2"></a>
<a class="sourceLine" id="cb115-3" data-line-number="3"><span class="co"># define the dynamic model</span></a>
<a class="sourceLine" id="cb115-4" data-line-number="4">LotVmod &lt;-<span class="st"> </span><span class="cf">function</span> (Time, State, Pars) {</a>
<a class="sourceLine" id="cb115-5" data-line-number="5">    <span class="kw">with</span>(<span class="kw">as.list</span>(<span class="kw">c</span>(State, Pars)), {</a>
<a class="sourceLine" id="cb115-6" data-line-number="6">        dx1 =<span class="st"> </span>alpha<span class="op">*</span>x1 <span class="op">-</span><span class="st"> </span>beta<span class="op">*</span>x1<span class="op">*</span>x2</a>
<a class="sourceLine" id="cb115-7" data-line-number="7">        dx2 =<span class="st"> </span>delta<span class="op">*</span>x1<span class="op">*</span>x2<span class="op">-</span><span class="st"> </span>gamma<span class="op">*</span>x2</a>
<a class="sourceLine" id="cb115-8" data-line-number="8">        <span class="kw">return</span>(<span class="kw">list</span>(<span class="kw">c</span>(dx1, dx2)))</a>
<a class="sourceLine" id="cb115-9" data-line-number="9">    })</a>
<a class="sourceLine" id="cb115-10" data-line-number="10">}</a>
<a class="sourceLine" id="cb115-11" data-line-number="11"></a>
<a class="sourceLine" id="cb115-12" data-line-number="12"><span class="co">#specify the model parameters, the initial populations, and the time course</span></a>
<a class="sourceLine" id="cb115-13" data-line-number="13">Pars &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">alpha =</span> <span class="dv">30</span>, <span class="dt">beta =</span> <span class="dv">5</span>, <span class="dt">gamma =</span> <span class="dv">2</span>, <span class="dt">delta =</span> <span class="dv">6</span>)</a>
<a class="sourceLine" id="cb115-14" data-line-number="14">State &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">x1 =</span> <span class="fl">8.792889e-3</span>, <span class="dt">x2 =</span> <span class="fl">1.595545</span>)</a>
<a class="sourceLine" id="cb115-15" data-line-number="15">Time &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="dt">by =</span> <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb115-16" data-line-number="16"></a>
<a class="sourceLine" id="cb115-17" data-line-number="17"><span class="co">#simulate the model</span></a>
<a class="sourceLine" id="cb115-18" data-line-number="18">out &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">ode</span>(<span class="dt">func =</span> LotVmod, <span class="dt">y =</span> State, <span class="dt">parms =</span> Pars, <span class="dt">times =</span> Time))</a>
<a class="sourceLine" id="cb115-19" data-line-number="19"></a>
<a class="sourceLine" id="cb115-20" data-line-number="20"><span class="co">#plot the output</span></a>
<a class="sourceLine" id="cb115-21" data-line-number="21"><span class="kw">plot</span>(out<span class="op">$</span>x2<span class="op">~</span>out<span class="op">$</span>time, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">xlab =</span> <span class="st">&quot;Time&quot;</span>, <span class="dt">ylab =</span> <span class="st">&quot;Population Density&quot;</span>, <span class="dt">main =</span> <span class="st">&quot;Predatory-Prey Model Simulation&quot;</span>, <span class="dt">col =</span> <span class="st">&quot;red&quot;</span>, <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">20</span>))</a>
<a class="sourceLine" id="cb115-22" data-line-number="22"><span class="kw">points</span>(out<span class="op">$</span>x1<span class="op">~</span>out<span class="op">$</span>time, <span class="dt">type =</span> <span class="st">&quot;l&quot;</span>, <span class="dt">add =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb115-23" data-line-number="23"><span class="kw">legend</span>(<span class="fl">3.8</span>,<span class="dv">20</span>,<span class="dt">legend =</span> <span class="kw">c</span>(<span class="st">&quot;Prey&quot;</span>, <span class="st">&quot;Predator&quot;</span>), <span class="dt">col =</span> <span class="kw">c</span>(<span class="st">&quot;red&quot;</span>, <span class="st">&quot;black&quot;</span>), <span class="dt">pch =</span> <span class="dv">20</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/simulate-and-plot-lotka-volterra-1.png" title="Simulation of Lotka-Volterra system" alt="Simulation of Lotka-Volterra system" width="672" /></p>
<p>
The dataset shown below corresponds to observations of an oscillatory predator-prey system. To calibrate the Lotka-Volterra model to this data we seek values of the model parameters for which simulation of the model provides the best fit. As in the regression tasks described previously, the standard measure for quality of fit is the sum of squared errors. We thus proceed with a minimization task: for each choice of numerical values for the model parameters, we compare model simulation with the data and determine the sum of squared errors. We aim to minimize this measure of fit over the space of model parameters.
</p>
<p><img src="_main_files/figure-html/generate-and-plot-lotka-volterra-corrupted-data-1.png" title="Data for Lotka-Volterra system" alt="Data for Lotka-Volterra system" width="672" /></p>
<div id="example-11" class="section level4">
<h4><span class="header-section-number">8.5.0.1</span> Example 11</h4>
<p>To illustrate, we’ll consider the case that two of the model parameter values have been established independently: <span class="math inline">\(\alpha= 30\)</span> and <span class="math inline">\(\delta=6\)</span>. We’ll then calibrate the model by estimating values for parameters <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span>. We begin by building a sum-of-squared errors function that takes values of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> as arguments. We will use the observed population values at time zero as initial condition for the simulations.</p>
<div class="sourceCode" id="cb116"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb116-1" data-line-number="1"><span class="co">#define SSE function</span></a>
<a class="sourceLine" id="cb116-2" data-line-number="2"><span class="kw">library</span>(deSolve)</a>
<a class="sourceLine" id="cb116-3" data-line-number="3">determine_sse &lt;-<span class="st"> </span><span class="cf">function</span>(p) {</a>
<a class="sourceLine" id="cb116-4" data-line-number="4"></a>
<a class="sourceLine" id="cb116-5" data-line-number="5">  <span class="co">#inputs are the two unknown model parameters collected into a vector p=[beta, gamma]</span></a>
<a class="sourceLine" id="cb116-6" data-line-number="6">  newPars &lt;-<span class="st"> </span><span class="kw">c</span>(p[<span class="dv">1</span>],p[<span class="dv">2</span>])</a>
<a class="sourceLine" id="cb116-7" data-line-number="7">  <span class="co">#initial populations are the observed values at time zero for x and y, respectively</span></a>
<a class="sourceLine" id="cb116-8" data-line-number="8">  newState &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">x1 =</span> <span class="fl">8.792889e-3</span>, <span class="dt">x2 =</span> <span class="fl">1.595545</span>)</a>
<a class="sourceLine" id="cb116-9" data-line-number="9"></a>
<a class="sourceLine" id="cb116-10" data-line-number="10">  <span class="co">#define the time-grid (same as above)</span></a>
<a class="sourceLine" id="cb116-11" data-line-number="11">  Time &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="dv">0</span>, <span class="dv">6</span>, <span class="dt">by =</span> <span class="fl">0.01</span>)</a>
<a class="sourceLine" id="cb116-12" data-line-number="12">  t_obs&lt;-<span class="kw">c</span>(<span class="fl">0.0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span>, <span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="fl">1.6</span>, <span class="fl">1.8</span>, <span class="fl">2.0</span>, <span class="fl">2.2</span>, <span class="fl">2.4</span>, <span class="fl">2.6</span>, <span class="fl">2.8</span>, <span class="fl">3.0</span>, <span class="fl">3.2</span>, <span class="fl">3.4</span>, <span class="fl">3.6</span>, <span class="fl">3.8</span>, <span class="fl">4.0</span>, <span class="fl">4.2</span>, <span class="fl">4.4</span>, <span class="fl">4.6</span>, <span class="fl">4.8</span>, <span class="fl">5.0</span>, <span class="fl">5.2</span>, <span class="fl">5.4</span>, <span class="fl">5.6</span>, <span class="fl">5.8</span>, <span class="fl">6.0</span>)</a>
<a class="sourceLine" id="cb116-13" data-line-number="13"></a>
<a class="sourceLine" id="cb116-14" data-line-number="14">  <span class="co">#dynamics as before</span></a>
<a class="sourceLine" id="cb116-15" data-line-number="15">newLotVmod &lt;-<span class="st"> </span><span class="cf">function</span> (Time, State, newPars) {</a>
<a class="sourceLine" id="cb116-16" data-line-number="16">    <span class="kw">with</span>(<span class="kw">as.list</span>(<span class="kw">c</span>(State, newPars)), {</a>
<a class="sourceLine" id="cb116-17" data-line-number="17">        dx1 =<span class="st"> </span>x1<span class="op">*</span>(<span class="dv">30</span> <span class="op">-</span><span class="st"> </span>p[<span class="dv">1</span>]<span class="op">*</span>x2)</a>
<a class="sourceLine" id="cb116-18" data-line-number="18">        dx2 =<span class="st"> </span><span class="op">-</span>x2<span class="op">*</span>(p[<span class="dv">2</span>] <span class="op">-</span><span class="st"> </span><span class="dv">6</span><span class="op">*</span>x1)</a>
<a class="sourceLine" id="cb116-19" data-line-number="19">        <span class="kw">return</span>(<span class="kw">list</span>(<span class="kw">c</span>(dx1, dx2)))</a>
<a class="sourceLine" id="cb116-20" data-line-number="20">    })</a>
<a class="sourceLine" id="cb116-21" data-line-number="21">}</a>
<a class="sourceLine" id="cb116-22" data-line-number="22"></a>
<a class="sourceLine" id="cb116-23" data-line-number="23">  <span class="co">#run the simulation with the user-specified values for beta and gamma</span></a>
<a class="sourceLine" id="cb116-24" data-line-number="24">  new_out &lt;-<span class="st"> </span><span class="kw">as.data.frame</span>(<span class="kw">ode</span>(<span class="dt">func =</span> newLotVmod, <span class="dt">y =</span> newState, <span class="dt">parms =</span> newPars, <span class="dt">times =</span> Time))</a>
<a class="sourceLine" id="cb116-25" data-line-number="25"></a>
<a class="sourceLine" id="cb116-26" data-line-number="26">  <span class="co">#generate vector of predictions to align with vector of observations</span></a>
<a class="sourceLine" id="cb116-27" data-line-number="27">  x1_pred&lt;-new_out<span class="op">$</span>x1[<span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">length</span>(new_out<span class="op">$</span>x1),<span class="dv">20</span>)]</a>
<a class="sourceLine" id="cb116-28" data-line-number="28">  x2_pred&lt;-new_out<span class="op">$</span>x2[<span class="kw">seq</span>(<span class="dv">1</span>,<span class="kw">length</span>(new_out<span class="op">$</span>x2),<span class="dv">20</span>)]</a>
<a class="sourceLine" id="cb116-29" data-line-number="29"></a>
<a class="sourceLine" id="cb116-30" data-line-number="30">  sse&lt;-<span class="kw">sum</span>((x1_obs_data <span class="op">-</span><span class="st"> </span>x1_pred)<span class="op">^</span><span class="dv">2</span>) <span class="op">+</span><span class="st"> </span><span class="kw">sum</span>((x2_obs_data <span class="op">-</span><span class="st"> </span>x2_pred)<span class="op">^</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb116-31" data-line-number="31">  <span class="kw">return</span>(sse)</a>
<a class="sourceLine" id="cb116-32" data-line-number="32">}</a></code></pre></div>
<p>We now call an optimization routine to search the space of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> values to minimize this SSE function. We apply the simulated annealing function <code>GenSA</code> as described above, with initial guess of <span class="math inline">\(\beta=1\)</span>, <span class="math inline">\(\gamma=1\)</span>:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1"><span class="kw">library</span>(GenSA)</a>
<a class="sourceLine" id="cb117-2" data-line-number="2">p1 =<span class="st"> </span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>)</a>
<a class="sourceLine" id="cb117-3" data-line-number="3">lower &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.1</span>,<span class="fl">0.1</span>)</a>
<a class="sourceLine" id="cb117-4" data-line-number="4">upper &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dv">10</span>,<span class="dv">10</span>)</a>
<a class="sourceLine" id="cb117-5" data-line-number="5">out0 &lt;-<span class="st"> </span><span class="kw">GenSA</span>(<span class="dt">par =</span> p1, <span class="dt">lower =</span> lower, <span class="dt">upper =</span> upper,<span class="dt">fn =</span> determine_sse, <span class="dt">control =</span> <span class="kw">list</span>(<span class="dt">maxit =</span> <span class="dv">4</span>))</a>
<a class="sourceLine" id="cb117-6" data-line-number="6">out0[<span class="kw">c</span>(<span class="st">&quot;value&quot;</span>,<span class="st">&quot;par&quot;</span>)]</a></code></pre></div>
<pre><code>$value
[1] 45.34987

$par
[1] 4.915015 2.018205</code></pre>
<p>The search algorithm has identified values of <span class="math inline">\(\beta=4.915\)</span> and <span class="math inline">\(\gamma=2.018\)</span> as minimizer, with the minimal sum of squares error value of 45.58991.</p>
<p>The best fit model simulation is shown along with the data below:</p>
<img src="_main_files/figure-html/plot-lotka-volterra-best-fit-and-data-1.png" title="Data for Lotka-Volterra system and best-fit simulation" alt="Data for Lotka-Volterra system and best-fit simulation" width="672" />
<p>
<p>A comprehensive discussion of calibration and uncertainty analysis of dynamic biological models can be found in (Ashyraliyev <em>et al.</em> 2009).</p>
</div>
<div id="exercise-3" class="section level4">
<h4><span class="header-section-number">8.5.0.2</span> Exercise 3</h4>
<p>
Following the process in Example 11, calibrate all four model parameters <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\gamma\)</span>, and <span class="math inline">\(\delta\)</span> to the following dataset, with initial states <span class="math inline">\(x_1(0)=0.01\)</span> and <span class="math inline">\(x_2(0)=1.0\)</span>. Use either simulated annealing or a genetic algorithm. You can generate a simulation with the predictions at the time-points corresponding to the observations by setting <code>Time &lt;- seq(0, 8, by = .2)</code> in the simulation script above. If you have trouble finding a suitable initial guess, try <span class="math inline">\(\alpha=10\)</span>, <span class="math inline">\(\beta=1\)</span>, <span class="math inline">\(\gamma=1\)</span>, <span class="math inline">\(\delta=1\)</span>.
</p>
<p><img src="_main_files/figure-html/exercise-3-generate-lotka-volterra-corrupted-data-1.png" title="Data for Lotka-Volterra system for exercise 3" alt="Data for Lotka-Volterra system for exercise 3" width="672" /></p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" data-line-number="1">t_obs_ex&lt;-<span class="kw">c</span>(<span class="fl">0.0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span>, <span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="fl">1.6</span>, <span class="fl">1.8</span>, <span class="fl">2.0</span>, <span class="fl">2.2</span>, <span class="fl">2.4</span>, <span class="fl">2.6</span>, <span class="fl">2.8</span>, <span class="fl">3.0</span>, <span class="fl">3.2</span>, <span class="fl">3.4</span>, <span class="fl">3.6</span>, <span class="fl">3.8</span>, <span class="fl">4.0</span>, <span class="fl">4.2</span>, <span class="fl">4.4</span>, <span class="fl">4.6</span>, <span class="fl">4.8</span>, <span class="fl">5.0</span>, <span class="fl">5.2</span>, <span class="fl">5.4</span>, <span class="fl">5.6</span>, <span class="fl">5.8</span>, <span class="fl">6.0</span>, <span class="fl">6.2</span>, <span class="fl">6.4</span>, <span class="fl">6.6</span>, <span class="fl">6.8</span>, <span class="fl">7.0</span>, <span class="fl">7.2</span>, <span class="fl">7.4</span>, <span class="fl">7.6</span>, <span class="fl">7.8</span>, <span class="fl">8.0</span>)</a>
<a class="sourceLine" id="cb119-2" data-line-number="2"></a>
<a class="sourceLine" id="cb119-3" data-line-number="3">x1_obs_ex&lt;-<span class="kw">c</span>(<span class="fl">0.0010</span>,  <span class="fl">0.043</span>,  <span class="fl">0.18</span>,  <span class="fl">0.86</span>,  <span class="fl">3.6</span>,  <span class="fl">3.24</span>,  <span class="fl">0.13</span>, <span class="fl">0.0075</span>,  <span class="fl">0.00095</span>, <span class="fl">0.0079</span>,  <span class="fl">0.00012</span>,  <span class="fl">0.00012</span>,  <span class="fl">0.00015</span>,  <span class="fl">0.00026</span>, <span class="fl">0.00057</span>,  <span class="fl">0.0016</span>,  <span class="fl">0.0052</span>,  <span class="fl">0.021</span>,  <span class="fl">0.084</span>,  <span class="fl">0.39</span>,  <span class="fl">1.8</span>, <span class="fl">5.3</span>,  <span class="fl">0.72</span>,  <span class="fl">0.028</span>,  <span class="fl">0.0024</span>, <span class="fl">0.010</span>,  <span class="fl">0.00019</span>,  <span class="fl">0.00013</span>, <span class="fl">0.00013</span>,  <span class="fl">0.00019</span>,  <span class="fl">0.00037</span>,  <span class="fl">0.00095</span>,  <span class="fl">0.0062</span>,  <span class="fl">0.010</span>,  <span class="fl">0.040</span>, <span class="fl">0.18</span>,  <span class="fl">0.84</span>,  <span class="fl">3.6</span>,  <span class="fl">3.3</span>,  <span class="fl">0.13</span>, <span class="fl">0.0091</span>)</a>
<a class="sourceLine" id="cb119-4" data-line-number="4"></a>
<a class="sourceLine" id="cb119-5" data-line-number="5">x2_obs_ex&lt;-<span class="kw">c</span>(<span class="fl">0.67</span>,  <span class="fl">1.2</span>,  <span class="fl">5.1</span>, <span class="fl">0.78</span>, <span class="fl">0.13</span>,  <span class="fl">5.7</span>,  <span class="fl">8.2</span>,  <span class="fl">7.5</span>,  <span class="fl">5.6</span>, <span class="fl">4.4</span>,  <span class="fl">3.0</span>,  <span class="fl">5.3</span>,  <span class="fl">2.2</span>,  <span class="fl">1.9</span>,  <span class="fl">1.4</span>,  <span class="fl">1.4</span>,  <span class="fl">1.2</span>,  <span class="fl">1.1</span>, <span class="fl">2.3</span>, <span class="fl">1.6</span>, <span class="fl">0.25</span>,  <span class="fl">3.8</span>,  <span class="fl">8.4</span>,  <span class="fl">8.3</span>,  <span class="fl">6.4</span>,  <span class="fl">5.0</span>,  <span class="fl">4.5</span>, <span class="fl">1.9</span>,  <span class="fl">2.8</span>,  <span class="fl">2.9</span>,  <span class="fl">1.6</span>,  <span class="fl">2.0</span>,  <span class="fl">1.7</span>,  <span class="fl">4.2</span>,  <span class="fl">1.8</span>,  <span class="fl">2.7</span>, <span class="fl">1.3</span>,  <span class="fl">2.0</span>,  <span class="fl">6.8</span>,  <span class="fl">8.7</span>,  <span class="fl">7.4</span>)</a></code></pre></div>
<!-- SOLUTION -->
<!-- ```{r ex3-determine-sse, echo = FALSE} -->
<!-- determine_sse_ex3 <- function(p) { -->
<!--   #first four input parameters are the kinetic parameters of the model -->
<!--   newPars <- c(p[1],p[2],p[3],p[4]) -->
<!--   #last tw parameters are the initial populations for x and y, respectively -->
<!--   newState <- c(x = 0.01, y = 1) -->
<!--   #time-grid is the same as before, no need to redefine -->
<!--   Time <- seq(0, 8, by = .2) -->
<!--   #kinetics  -->
<!-- newLotVmod <- function (Time, State, newPars) { -->
<!--     with(as.list(c(State, newPars)), { -->
<!--         dx = x*(p[1] - p[2]*y) -->
<!--         dy = -y*(p[3] - p[4]*x) -->
<!--         return(list(c(dx, dy))) -->
<!--     }) -->
<!-- } -->
<!--   new_out1 <- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time)) -->
<!--   #generate vector of predictions to align with vector of observations -->
<!--   #predictions1<-data.frame(t_pred1<-times, -->
<!--    #               x_pred1<-new_out1$x, -->
<!--     #              y_pred1<-new_out1$y) -->
<!--   x_obs_ex<- c(0.0010,  0.043,  0.18,  0.86,  3.6,  3.24,  0.13, 0.0075,  0.00095, 0.0079,  0.00012,  0.00012,  0.00015,  0.00026, 0.00057,  0.0016,  0.0052,  0.021,  0.084,  0.39,  1.8, 5.3,  0.72,  0.028,  0.0024, 0.010,  0.00019,  0.00013, 0.00013,  0.00019,  0.00037,  0.00095,  0.0062,  0.010,  0.040, 0.18,  0.84,  3.6,  3.3,  0.13, 0.0091) -->
<!--   y_obs_ex <- c(0.67,  1.2,  5.1, 0.78, 0.13,  5.7,  8.2,  7.5,  5.6, 4.4,  3.0,  5.3,  2.2,  1.9,  1.4,  1.4,  1.2,  1.1, 2.3, 1.6, 0.25,  3.8,  8.4,  8.3,  6.4,  5.0,  4.5, 1.9,  2.8,  2.9,  1.6,  2.0,  1.7,  4.2,  1.8,  2.7, 1.3,  2.0,  6.8,  8.7,  7.4) -->
<!--   sse<-sum((x_obs_ex - new_out1$x)^2) + sum((y_obs_ex - new_out1$y)^2) -->
<!--   return(sse) -->
<!-- } -->
<!-- determine_sse_ex3(c(1,1,1,5)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- library(GenSA) -->
<!-- #x2 = c(10,3,1,2) -->
<!-- x2 = c(10,1,1,1) -->
<!-- lower <- c(0.1,0.1,0.1,0.1) -->
<!-- upper <- c(20,4,2,3) -->
<!-- out0 <- GenSA(par = x2, lower = lower, upper = upper,fn = determine_sse_ex3, control = list(maxit = 50)) -->
<!-- out0[c("value","par")] -->
<!-- ``` -->
<!-- #### Exercise X -->
<!-- <p> The behaviour of the Lotka-Volterra model described above depends on the values of the model parameters. Such models are often used to explore the effect of perturbations on a system. For instance, after successfully calibrating a model to given population's dynamics, one could propose interventions that would alter the value of one of the model parameters, e.g. by restricting access to resources. We could then simulate the model under this alteration and thus generate predictions of the effect of this manipulation. </p> -->
<!-- <p> Alternatively, we could use a model formulation that incorporates a time-varying perturbation.  As an example, consider a version of the Lotka-Volterra equations that includes a term to describe removal (harvesting/culling) of the predator population:  -->
<!--  \begin{eqnarray*}  -->
<!--      \frac{d}{dt} x_1(t) &=& \alpha x_1(t) - \beta x_1(t) x_2(t)\\  -->
<!--      \frac{d}{dt} x_2(t) &=& \gamma x_1(t) x_2(t) - \delta x_2(t) - u(t) x_2(t)  -->
<!--  \end{eqnarray*} </p>  -->
<!-- <p> Here $u(t)$, called an _input signal_, is a function that represents the effort exerted in removal of predators. With this input signal in place, we can now use the model to explore the consequences of a range of removal schedules. In particular, optimization can be used to identify the removal strategy that best achieves some performance goal. </p>  -->
<!-- <p> As a concrete example, consider the goal of maximizing harvest over some fixed time period. Suppose that the harvesting rate must be fixed throughout the time interval of interest so $u(t) = u_0$ is constant. We can account for the total harvest by introducing a new state variable that tracks the progress of the harvest: -->
<!-- \begin{eqnarray*} -->
<!--     \frac{d}{dt} x_1(t) &=& \alpha x_1(t) - \beta x_1(t) x_2(t)\\ -->
<!--     \frac{d}{dt} x_2(t) &=& \gamma x_1(t) x_2(t) - \delta x_2(t) - u_0 x_2(t)\\ -->
<!--     \frac{d}{dt} x_3(t) &=& u_0 x_2(t) -->
<!-- \end{eqnarray*} </p> -->
<!-- <p> Then, with $x_3(0)=0$, the value of $x_3(t)$ is the accumulated harvest from time 0 to time $t$. If our goal is to maximize the total harvest over an interval $[0, b]$, we aim to maximize the value of $x_3(b)$ over the choice of the fixed harvesting effort $u_0$. </p> -->
<!-- Take parameter values of $p_1 = 30$, $p_2 = 5$, $p_3 = 2$, $p_4 = 6$, and initial state $x_1(0) = 0.0088$, $x_2(0)=1.6$. Determine the value of $u_0$ that maximizes the total harvest over a period of $t \in [0, 6]$. -->
<!-- SOLUTION -->
<!-- ```{r exx-determine-sse, echo = FALSE} -->
<!-- determine_sse_ex4 <- function(u) { -->
<!--   #first four input parameters are the kinetic parameters of the model -->
<!--   newPars <- c(u) -->
<!--   #last two parameters are the initial populations for x and y, respectively -->
<!--   newState <- c(x = 0.0088, y = 1.6, z=0) -->
<!--   #time-grid is the same as before, no need to redefine -->
<!--   Time <- seq(0, 6, by = .0005) -->
<!--   #kinetics -->
<!-- newLotVmod <- function (Time, State, newPars) { -->
<!--     with(as.list(c(State, newPars)), { -->
<!--         dx = x*(30 - 5*y - 1*x) -->
<!--         dy = -y*((2+u) - 6*x) -->
<!--         dz = u*y -->
<!--         return(list(c(dx, dy, dz))) -->
<!--     }) -->
<!-- } -->
<!-- out <- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time, method = "radau", atol = 1e-6, rtol = 1e-6)) -->
<!-- plot(out$y~out$time, type = "l", xlab = "Time", ylab = "Population Density", main = "Predatory-Prey Model Simulation", col = "red", ylim = c(0,20)) -->
<!-- points(out$x~out$time, type = "l", add = TRUE) -->
<!-- legend(3.8,20,legend = c("Prey", "Predator"), col = c("red", "black"), pch = 20) -->
<!--   #generate vector of predictions to align with vector of observations -->
<!--   #predictions1<-data.frame(t_pred1<-times, -->
<!--    #               x_pred1<-new_out1$x, -->
<!--     #              y_pred1<-new_out1$y) -->
<!--   harvest <- tail(out$z, n=1) -->
<!--   return(harvest) -->
<!-- } -->
<!-- determine_sse_ex4(c(60)) -->
<!-- ``` -->
<!-- ```{r} -->
<!-- library(GenSA) -->
<!-- #x2 = c(10,3,1,2) -->
<!-- x2 = c(10,1,1,1) -->
<!-- lower <- c(0.1,0.1,0.1,0.1) -->
<!-- upper <- c(20,4,2,3) -->
<!-- out0 <- GenSA(par = x2, lower = lower, upper = upper,fn = determine_sse_ex3, control = list(maxit = 50)) -->
<!-- out0[c("value","par")] -->
<!-- ``` -->
<!-- <p> Let's now consider the case in which we can change the harvesting effort through the year: we'll allow the input $u(t)$ to be a time-varying function. One would expect we'd be able to achieve a larger harvest in this case (compared with a fixed harvest rate), because there's more freedom in the design of the harvesting strategy. We'll see below that this is indeed true. However, the optimization task as currently posed is problematic. So far, we've been optimizing over functions (SSE, total harvest) that depend on a finite set of numerical values, and so we searched a finite-dimensional space for the extrema. We're now faced with optimizing over *any* function $u(t)$. This is what's technically called an infinite-dimensional search space. The search for extrema in this case has been investigated in the theory of _optimal control_ and closely related work in the _calculus of variations_. See (Lenhart and Workman, 2007) if you'd like to learn more. Here, we'll restrict ourselves to a simple short-cut for solving this problem. We need to find a way to _parameterize_ the set of possible input curves $u(t)$. A common strategy is as follows: -->
<!-- __Sample-and-hold input parametrization__ -->
<!-- <p> We suppose that the input $u(t)$ (harvesting effort) is a piece-wise constant function, which changes values only at pre-specified timepoints (e.g. monthly, as shown in figure XYZ). Then we can specify any such function as a set of 12 numerical values, one for each month: ($u_1$, $u_2$, $u_3$, ..., $u_{11}$, $u_{12}$). As youd expect, the optimization task (which is now an optimal control task) can be solved by searching for the maximizer of total harvest over this 12-dimensional search space. More details in Lin et al. 2014. </p> -->
<!-- [Implementation]; note: easiest way to implement is as a series of separate simulations. Also note:  solution is not differentiable at timepoints where $u$ jumps. -->
<!-- exercise: change number of sub-intervals, note increase/decrease in maximum achieved] -->
</div>
</div>
<div id="uncertainty-analysis-and-bayesian-calibration" class="section level2">
<h2><span class="header-section-number">8.6</span> Uncertainty Analysis and Bayesian Calibration</h2>
<p>
So far, we’ve used regression analysis to provide specific estimates of parameter values. Regression is usually followed by <em>uncertainty analysis</em>, which provides some measure of confidence in those parameter value estimates. For instance, in the case of linear regression, 95% confidence intervals on the parameter estimates and corresponding confidence intervals on the model predictions are supported by extensive statistical theory, and can be readily generated in R, as follows. Consider again the linear regression fit from Example 4 above.
</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" data-line-number="1"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb120-2" data-line-number="2"><span class="kw">data</span>(<span class="st">&quot;iris&quot;</span>)</a>
<a class="sourceLine" id="cb120-3" data-line-number="3"></a>
<a class="sourceLine" id="cb120-4" data-line-number="4">iris.lm &lt;-<span class="st"> </span><span class="kw">lm</span>(Sepal.Length<span class="op">~</span><span class="st"> </span>Petal.Width, <span class="dt">data =</span> iris)</a>
<a class="sourceLine" id="cb120-5" data-line-number="5"></a>
<a class="sourceLine" id="cb120-6" data-line-number="6">new.dat &lt;-<span class="st"> </span><span class="kw">seq</span>(<span class="kw">min</span>(iris<span class="op">$</span>Petal.Width),<span class="kw">max</span>(iris<span class="op">$</span>Petal.Width),<span class="dt">by =</span> <span class="fl">0.05</span>)</a>
<a class="sourceLine" id="cb120-7" data-line-number="7">pred_interval &lt;-<span class="st"> </span><span class="kw">predict</span>(iris.lm, <span class="dt">newdata =</span> <span class="kw">data.frame</span>(<span class="dt">Petal.Width=</span>new.dat), <span class="dt">interval=</span><span class="st">&#39;prediction&#39;</span>,<span class="dt">level=</span><span class="fl">0.95</span>)</a></code></pre></div>
<p>In example 4, we found best fit model parameters of <span class="math inline">\(b=4.7776\)</span> and <span class="math inline">\(m=0.8886\)</span>. The plot below shows 95% confidence intervals on the model predictions.</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1"><span class="kw">plot</span>(iris<span class="op">$</span>Petal.Width,iris<span class="op">$</span>Sepal.Length,<span class="dt">xlab=</span><span class="st">&quot;Sepal Length&quot;</span>, <span class="dt">ylab=</span><span class="st">&quot;Petal Width&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Regression&quot;</span>,<span class="dt">pch=</span><span class="dv">16</span>)</a>
<a class="sourceLine" id="cb121-2" data-line-number="2"><span class="kw">abline</span>(iris.lm,<span class="dt">col=</span><span class="st">&quot;firebrick&quot;</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb121-3" data-line-number="3"><span class="kw">lines</span>(new.dat, pred_interval[,<span class="dv">2</span>], <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb121-4" data-line-number="4"><span class="kw">lines</span>(new.dat, pred_interval[,<span class="dv">3</span>], <span class="dt">col=</span><span class="st">&quot;steelblue&quot;</span>, <span class="dt">lty=</span><span class="dv">2</span>,<span class="dt">lwd=</span><span class="dv">2</span>)</a>
<a class="sourceLine" id="cb121-5" data-line-number="5"><span class="kw">legend</span>(<span class="dv">4</span>, <span class="dv">105</span>, <span class="dt">legend=</span><span class="kw">c</span>(<span class="st">&quot;Best-fit Line&quot;</span>, <span class="st">&quot;Prediction interval&quot;</span>),</a>
<a class="sourceLine" id="cb121-6" data-line-number="6">       <span class="dt">col=</span><span class="kw">c</span>(<span class="st">&quot;firebrick&quot;</span>, <span class="st">&quot;steelblue&quot;</span>), <span class="dt">lty=</span><span class="dv">1</span><span class="op">:</span><span class="dv">2</span>, <span class="dt">cex=</span><span class="fl">0.8</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/plot-prediction-confidence-intervals-linear-regression-iris-1.png" title="Prediction confidence intervals of linear regression fit" alt="Prediction confidence intervals of linear regression fit" width="672" /></p>
<p>The <code>confint</code> function determines 95% confidence intervals on the estimates of the parameters (the intercept <span class="math inline">\(b\)</span> and the slope <span class="math inline">\(m\)</span>).</p>
<pre><code>                2.5 %    97.5 %
(Intercept) 4.6335014 4.9217574
Petal.Width 0.7870598 0.9901007</code></pre>
<p>
For nonlinear regression (including calibration of dynamic models), the theory is less helpful, but estimates of uncertainty intervals can be achieved. Recall the nonlinear regression fit from Example 5 above, where the best-fit values were founds as <span class="math inline">\(K_M =0.4187090\)</span> and <span class="math inline">\(V_{\mbox{max}}=0.5331688\)</span>.
</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" data-line-number="1"><span class="co">#substrate concentrations</span></a>
<a class="sourceLine" id="cb123-2" data-line-number="2">S &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">4.32</span>, <span class="fl">2.16</span>, <span class="fl">1.08</span>, <span class="fl">0.54</span>, <span class="fl">0.27</span>, <span class="fl">0.135</span>, <span class="fl">3.6</span>, <span class="fl">1.8</span>, <span class="fl">0.9</span>, <span class="fl">0.45</span>, <span class="fl">0.225</span>, <span class="fl">0.1125</span>, <span class="fl">2.88</span>, <span class="fl">1.44</span>, <span class="fl">0.72</span>, <span class="fl">0.36</span>, <span class="fl">0.18</span>, <span class="fl">0.9</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb123-3" data-line-number="3"><span class="co">#reaction velocities</span></a>
<a class="sourceLine" id="cb123-4" data-line-number="4">V &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.48</span>, <span class="fl">0.42</span>, <span class="fl">0.36</span>, <span class="fl">0.26</span>, <span class="fl">0.17</span>, <span class="fl">0.11</span>, <span class="fl">0.44</span>, <span class="fl">0.47</span>, <span class="fl">0.39</span>, <span class="fl">0.29</span>, <span class="fl">0.18</span>, <span class="fl">0.12</span>, <span class="fl">0.50</span>, <span class="fl">0.45</span>, <span class="fl">0.37</span>, <span class="fl">0.28</span>, <span class="fl">0.19</span>, <span class="fl">0.31</span>, <span class="dv">0</span>)</a>
<a class="sourceLine" id="cb123-5" data-line-number="5">MM_data=<span class="kw">cbind</span>(S,V)</a>
<a class="sourceLine" id="cb123-6" data-line-number="6">MMmodel.nls &lt;-<span class="st"> </span><span class="kw">nls</span>(V <span class="op">~</span><span class="st"> </span>Vmax<span class="op">*</span>S<span class="op">/</span>(Km<span class="op">+</span>S),<span class="dt">start =</span> <span class="kw">list</span>(<span class="dt">Km=</span><span class="fl">0.3</span>, <span class="dt">Vmax=</span><span class="fl">0.5</span>)) <span class="co">#perform the nonlinear regression analysis</span></a>
<a class="sourceLine" id="cb123-7" data-line-number="7">params &lt;-<span class="st"> </span><span class="kw">summary</span>(MMmodel.nls)<span class="op">$</span>coeff[,<span class="dv">1</span>] <span class="co">#extract the parameter estimates</span></a>
<a class="sourceLine" id="cb123-8" data-line-number="8"><span class="kw">plot</span>(MM_data)</a>
<a class="sourceLine" id="cb123-9" data-line-number="9"><span class="kw">curve</span>((params[<span class="dv">2</span>]<span class="op">*</span>x)<span class="op">/</span>(params[<span class="dv">1</span>]<span class="op">+</span>x), <span class="dt">from =</span> <span class="dv">0</span>, <span class="dt">to =</span> <span class="fl">4.5</span>, <span class="dt">add=</span><span class="ot">TRUE</span>, <span class="dt">col=</span><span class="st">&#39;firebrick&#39;</span>)</a></code></pre></div>
<p><img src="_main_files/figure-html/2plot-Michaelis-Menten-dataset-1.png" title="Nonlinear regression fit from example 5" alt="Nonlinear regression fit from example 5" width="672" /></p>
<p>Applying the <code>confit</code> function generates confidence intervals for the parameter estimates</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1"><span class="kw">confint</span>(MMmodel.nls)</a></code></pre></div>
<pre><code>Waiting for profiling to be done...</code></pre>
<pre><code>          2.5%     97.5%
Km   0.3143329 0.5528199
Vmax 0.4899962 0.5816347</code></pre>
<!-- ```{r} -->
<!-- lm_summ <- summary(model.nls) -->
<!-- c("lower" = lm_summ$coef[2,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2], -->
<!--   "upper" = lm_summ$coef[2,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[2, 2]) -->
<!-- c("lower" = lm_summ$coef[1,1] - qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[1, 2], -->
<!--   "upper" = lm_summ$coef[1,1] + qt(0.975, df = lm_summ$df[2]) * lm_summ$coef[1, 2]) -->
<!-- ``` -->
<p>
<strong>Bayesian methods</strong> address the regression task by combining calibration and uncertainty in a single process. The basic idea behind Bayesian analysis (founded on Bayes Theorem, which may be familiar from elementary probability), is to start with some knowledge about the parameter estimates (analogous to the initial guess supplied to nonlinear regression) and then use the available data to refine that knowledge. In the Bayesian context, instead of the initial guess and refined estimate being single numerical values, they are <em>distributions</em>. In Bayesian terminology, we being with a <em>prior</em> distribution, which may be based on previously established expert knowledge. A commonly used prior is a normal distribution centered at a good initial estimate. In other cases the prior may be a uniform distribution over a wide range of possible values, indicating minimal previously established knowledge about the parameter values. Application of a Bayesian calibration scheme uses the available data to generate an improved distribution of the parameter values, which is called the <em>posterior</em> distribution. A successful Bayesian calibration could take a `wild guess’ uniform prior and return a tightly-centered posterior. Uncertainty can then be gleaned directly from the posterior distribution.
</p>
<div id="example-12" class="section level4">
<h4><span class="header-section-number">8.6.0.1</span> Example 12</h4>
<p>
Here we’ll consider a straightforward implementation of a Bayesian approach: Approximate Bayesian Computation (ABC). This approach is based on a simple idea: the <em>rejection method</em>, in which we sample repeatedly from the prior distribution and reject all samples that do not satisfy a pre-specified tolerance for quality of fit. (This is reminiscent of the selection step in genetic algorithms: culling unfit members of a population.) The samples that are not rejected form the <em>posterior</em> distribution.
</p>
<p>To illustrate, we’ll revisit the Michaelis-Menten data from Example 5 above. We select uniform priors for both parameters: <span class="math inline">\(K_M\)</span> and <span class="math inline">\(V_{\mbox{max}}\)</span> both uniform on <span class="math inline">\([0,1]\)</span>.</p>
<!-- ```{r} -->
<!-- true_data<-V -->
<!-- acceptance_threshold<-0.01 -->
<!-- Km <- draw_Km() -->
<!--     Vmax <- draw_Vmax() -->
<!--     simulated_data <- simulate_data(S,Km,Vmax) -->
<!--     Km -->
<!--     Vmax -->
<!--     simulated_data -->
<!--     true_data -->
<!--     distance <- compare_with_squared_distance(true_data, simulated_data) -->
<!--     distance -->
<!--     if (distance < acceptance_threshold) { -->
<!--       accepted_or_rejected <- 1 -->
<!--     } else { -->
<!--       accepted_or_rejected <- 0 -->
<!--       } -->
<!--     #accepted_or_rejected <-accept_or_reject_with_squared_distance(true_data, simulated_data, acceptance_threshold) -->
<!--     accepted_or_rejected -->
<!-- ``` -->
<p>In the call below, we sample from the prior distributions of <span class="math inline">\(K_M\)</span> and <span class="math inline">\(V_{\mbox{max}}\)</span> 200000 times. We set the acceptance threshold as 0.15. That is, parameter pairs that give rise to SSE values below 0.15 are accepted; others are rejected. The histograms below show the uniform priors along with the posteriors. The algorithm has successfully tightened the distributions about the best-fit parameter estimates established above (<span class="math inline">\(K_M =0.4187090\)</span> and <span class="math inline">\(V_{\mbox{max}}=0.5331688\)</span>).</p>
<pre><code>[1] 2225</code></pre>
<p>We see that only 2225 of 200 000 samples were accepted, yielding an acceptance rate of about 1%. The acceptance rate can be tuned by choice of the rejection threshold. A low acceptance rate gives rise to a computationally expensive algorithm, while a high acceptance rate can lead to poor estimation.</p>
<p><img src="_main_files/figure-html/plot-rejection-histograms-1.png" title="Posterior histograms for Michaelis-Menten fit" alt="Posterior histograms for Michaelis-Menten fit" width="672" /><img src="_main_files/figure-html/plot-rejection-histograms-2.png" title="Posterior histograms for Michaelis-Menten fit" alt="Posterior histograms for Michaelis-Menten fit" width="672" /></p>
<p>
Typically, approximate Bayesian computation is implemented as a sequential method, in which a sequence of rejection steps is applied, with the distribution being refined at each step (generating, in essence, a sequence of posterior distributions, the last of which is considered to be the best description of the desired parameter estimates). The <code>EasyABC</code> package can be used to implement sequential ABC. (More details on this package can be found in (Beaumont, 2019) and <a href="https://cran.r-project.org/web/packages/EasyABC/EasyABC.pdf">here</a>.)
</p>
<!-- ```{r set-seed-for-EasyABC} -->
<!-- # set seed -->
<!-- set.seed(123) -->
<!-- ``` -->
<!-- ```{r call-Easy_ABC, message=FALSE,warning=FALSE} -->
<!-- library(EasyABC) -->
<!-- # simulate data -->
<!-- mm_model <- function(par) { -->
<!--   Vmax <- par[1] -->
<!--   Km <- par[2] -->
<!--   S <- S -->
<!--   samples <- S*Vmax/(S+Km) -->
<!--   return(samples) -->
<!-- } -->
<!-- # set tolerance levels -->
<!-- tolerance=c(8,3) -->
<!-- # define prior distribution for parameters -->
<!-- mm_prior = list(c("unif",0,0.01),c("unif",0,1)) -->
<!-- # use ABC_sequential() to obtain a posterior distribution of parameter values  !!!20000 reduced to 100 for kintting time -->
<!-- ABC_sequential <- ABC_sequential(method="Beaumont", model=mm_model,prior=mm_prior, -->
<!--                                  nb_simul=10000, summary_stat_target=V, -->
<!--                                  tolerance_tab=tolerance) -->
<!-- ``` -->
<!-- After 20000 simulations, it can be seen from the histograms that the most frequent values are close to the estimated values derived from the non linear regression model. -->
<!-- ```{r} -->
<!-- c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue") -->
<!-- c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink") -->
<!-- #hist(runif(15000,0,0.01),    col = c1,xlab = "Km") -->
<!-- hist(ABC_sequential$param[1:10000,1],main="Prior and Posterior Distributions for Km",col = c2, xlab = "Km", xlim=c(0,0.01)) -->
<!-- rect(0, 0, 0.01, 80, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- #abline(V=0.005425,col="firebrick",lwd=2) -->
<!-- legend("topleft",legend=c("Prior Distribution for Km", "Posterior Distribution for Km"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- #hist(runif(10000,0,1),main="Prior and Posterior Distributions for Vmax",     col = c1,xlab = "Vmax") -->
<!-- hist(ABC_sequential$param[1:10000,2],main="Prior and Posterior Distributions for Vmax", col = c2, xlab = "Vmax", xlim=c(0,1)) -->
<!-- rect(0, 0, 1, 20, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- #abline(v=0.439802,col="firebrick",lwd=2) -->
<!-- legend("topleft",legend=c("Prior Distribution for Vm", "Posterior Distribution for Vmax"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- ``` -->
</div>
<div id="exercise-4" class="section level4">
<h4><span class="header-section-number">8.6.0.2</span> Exercise 4</h4>
<p>
Consider the task of finding the minimum of the function <span class="math inline">\(f(x,y)=x^4 + y^2\)</span>, plotted below. Clearly, the minimum is zero, and numerical optimization routines will have no trouble estimating that solution. However, because the curvature at the minimum is much different along the <span class="math inline">\(x\)</span>- and <span class="math inline">\(y\)</span>-directions, most numerical algorithms will do a better job estimating one parameter compared to the other. This can be well-illustrated by applying the rejection method to this problem and noting the relative spread in the posterior distributions. Implement the rejection algorithm provided above with uniform prior distributions on [-1, 1] for both the <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> values. Choose a rejection threshold that provides an acceptance rate of about 1%. Which estimate is provided with more confidence (i.e. which posterior is more tightly distributed? How can you relate that back to the curvature of the function at the minimum?.
</p>
<p><img src="_main_files/figure-html/plot-wireframe-ex5-function-1.png" width="672" /></p>
<!-- SOLUTION Much tighter in x than y, as expected.-->
<!-- ```{r rejection-method-for-ex-4, ECHO=TRUE} -->
<!-- # recall the dataset -->
<!-- # define functions to draw values for Km and Vmax from the prior distributions -->
<!-- draw_x <- function () { -->
<!--   return (runif(1, min=-1, max=1)) -->
<!-- } -->
<!-- draw_y <- function () { -->
<!--   return (runif(1, min=-1, max=1)) -->
<!-- } -->
<!-- # sampling algorithm: returns the sampled Km and Vmax values and whether this pair is accepted -->
<!-- sample_by_rejection <- function (n_iterations, acceptance_threshold) { -->
<!--   accepted <- vector(length = n_iterations) -->
<!--   sampled_x <- vector(length = n_iterations, mode = "numeric") -->
<!--   sampled_y <- vector (length = n_iterations, mode = "numeric") -->
<!--   for (i in 1:n_iterations){ -->
<!--     x <- draw_x() -->
<!--     y <- draw_y() -->
<!--     if (x^4+y^2 < acceptance_threshold) { -->
<!--       accepted[i] <- 1 -->
<!--     } else { -->
<!--       accepted[i] <- 0 -->
<!--       } -->
<!--     #accepted_or_rejected[i] = accept_or_reject_function(true_data, simulated_data, acceptance_threshold) -->
<!--     sampled_x[i] = x -->
<!--     sampled_y[i] = y -->
<!--   } -->
<!--   return(data.frame(cbind("accepted" = accepted, "sampled_xs" = sampled_x, "sampled_ys" = sampled_y))) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- # set seed -->
<!-- set.seed(132) -->
<!-- # simulate 200000 times with a threshold of 0.15  -->
<!-- sampled_parameter_values_squared_distances = sample_by_rejection(20000, 0.005) -->
<!-- # report the number of accepted values among the 200000 samples -->
<!-- sum(sampled_parameter_values_squared_distances$accepted) -->
<!-- ``` -->
<!-- ```{r plot-rejection-histograms, echo=FALSE} -->
<!-- c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue") -->
<!-- c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink") -->
<!-- post1 <- hist(sampled_parameter_values_squared_distances[which(sampled_parameter_values_squared_distances$accepted==1),3], col=c2, xlab = "x", xlim=c(-1,1), main='') -->
<!-- rect(-1, 0, 1, 35, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- legend("topright",legend=c("Prior Distribution for x", "Posterior Distribution for x"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- #prior2 = hist(sampled_parameter_values_squared_distances[1:2500,2],col = c1, -->
<!-- #              xlab = "Vm", main="Prior Distribution for Vm") -->
<!-- post2 = hist(sampled_parameter_values_squared_distances[which(sampled_parameter_values_squared_distances$accepted==1),2],col = c2, xlab = "y", xlim=c(-1,1),main='') -->
<!-- rect(-1, 0, 1, 50, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- legend("topright",legend=c("Prior Distribution for y", "Posterior Distribution for y"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- ``` -->
</div>
<div id="exercise-5" class="section level4">
<h4><span class="header-section-number">8.6.0.3</span> Exercise 5</h4>
<p>
Repeat example 11 by applying the rejection algorithm to estimate the values of <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\gamma\)</span> by fitting the Lotka-Volterra model to the data provided below. Use initial conditions <span class="math inline">\(x = 0.008792889\)</span>, <span class="math inline">\(y = 1.595545\)</span> as in the example. Use uniform prior distributions of [2,30] for <span class="math inline">\(\beta\)</span> and [1,4] for <span class="math inline">\(\gamma\)</span>, and choose a rejection threshold that provides an acceptance rate of about 1%. Do you find that one parameter is more confidently estimated (tighter distribution) than the other? Use the data shown below. To avoid errors in the simulation, you can add the options <code>method = &quot;radau&quot;, atol = 1e-6, rtol = 1e-6</code> to the call to <code>ode</code>.
</p>
<div class="sourceCode" id="cb128"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb128-1" data-line-number="1">t_obs_ex5&lt;-<span class="kw">c</span>(<span class="fl">0.0</span>, <span class="fl">0.2</span>, <span class="fl">0.4</span>, <span class="fl">0.6</span>, <span class="fl">0.8</span>, <span class="fl">1.0</span>, <span class="fl">1.2</span>, <span class="fl">1.4</span>, <span class="fl">1.6</span>, <span class="fl">1.8</span>, <span class="fl">2.0</span>, <span class="fl">2.2</span>, <span class="fl">2.4</span>, <span class="fl">2.6</span>, <span class="fl">2.8</span>, <span class="fl">3.0</span>, <span class="fl">3.2</span>, <span class="fl">3.4</span>, <span class="fl">3.6</span>, <span class="fl">3.8</span>, <span class="fl">4.0</span>, <span class="fl">4.2</span>, <span class="fl">4.4</span>, <span class="fl">4.6</span>, <span class="fl">4.8</span>, <span class="fl">5.0</span>, <span class="fl">5.2</span>, <span class="fl">5.4</span>, <span class="fl">5.6</span>, <span class="fl">5.8</span>, <span class="fl">6.0</span>, <span class="fl">6.2</span>, <span class="fl">6.4</span>, <span class="fl">6.6</span>, <span class="fl">6.8</span>, <span class="fl">7.0</span>, <span class="fl">7.2</span>, <span class="fl">7.4</span>, <span class="fl">7.6</span>, <span class="fl">7.8</span>, <span class="fl">8.0</span>)</a>
<a class="sourceLine" id="cb128-2" data-line-number="2"></a>
<a class="sourceLine" id="cb128-3" data-line-number="3">x_obs_ex5 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.0091</span>, <span class="fl">0.043</span>, <span class="fl">0.20</span>, <span class="fl">0.82</span>, <span class="fl">3.4</span>, <span class="fl">2.9</span>, <span class="fl">0.12</span>, <span class="fl">0.0077</span>, <span class="fl">0.00093</span>, <span class="fl">0.00026</span>, <span class="fl">0.00014</span>, <span class="fl">0.00013</span>, <span class="fl">0.00015</span>, <span class="fl">0.00025</span>, <span class="fl">0.00054</span>, <span class="fl">0.0015</span>, <span class="fl">0.0051</span>, <span class="fl">0.020</span>, <span class="fl">0.087</span>, <span class="fl">0.36</span>, <span class="fl">1.7</span>, <span class="fl">5.6</span>, <span class="fl">0.66</span>, <span class="fl">0.029</span>, <span class="fl">0.0024</span>, <span class="fl">0.00048</span>, <span class="fl">0.00019</span>, <span class="fl">0.00012</span>, <span class="fl">0.00013</span>, <span class="fl">0.00019</span>, <span class="fl">0.00035</span>, <span class="fl">0.0010</span>, <span class="fl">0.0030</span>, <span class="fl">0.010</span>, <span class="fl">0.041</span>, <span class="fl">0.19</span>, <span class="fl">0.88</span>, <span class="fl">3.8</span>, <span class="fl">3.2</span>, <span class="fl">0.13</span>, <span class="fl">0.0078</span>)</a>
<a class="sourceLine" id="cb128-4" data-line-number="4"></a>
<a class="sourceLine" id="cb128-5" data-line-number="5">y_obs_ex5 &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="fl">0.82</span>, <span class="fl">1.1</span>, <span class="fl">0.98</span>, <span class="fl">0.65</span>, <span class="fl">1.1</span>, <span class="fl">5.1</span>, <span class="fl">7.1</span>, <span class="fl">7.9</span>, <span class="fl">4.3</span>, <span class="fl">3.5</span>, <span class="fl">3.8</span>, <span class="fl">3.9</span>, <span class="fl">2.5</span>, <span class="fl">2.1</span>, <span class="fl">1.6</span>, <span class="fl">1.4</span>, <span class="fl">1.2</span>, <span class="fl">1.0</span>, <span class="fl">0.92</span>, <span class="fl">0.57</span>, <span class="fl">0.73</span>, <span class="fl">3.6</span>, <span class="fl">6.7</span>, <span class="fl">9.1</span>, <span class="fl">5.5</span>, <span class="fl">4.7</span>, <span class="fl">4.5</span>, <span class="fl">3.0</span>, <span class="fl">2.9</span>, <span class="fl">2.7</span>, <span class="fl">1.7</span>, <span class="fl">2.0</span>, <span class="fl">1.7</span>, <span class="fl">1.3</span>, <span class="fl">0.98</span>, <span class="fl">0.98</span>, <span class="fl">0.84</span>, <span class="fl">1.6</span>, <span class="fl">6.5</span>, <span class="fl">8.2</span>, <span class="fl">7.7</span>)</a></code></pre></div>
<!-- SOLUTION -->
<!-- ```{r ex5-determine-sse, echo = FALSE} -->
<!-- library(deSolve) -->
<!-- determine_sse_ex5 <- function(p) { -->
<!--   #first four input parameters are the kinetic parameters of the model -->
<!--   newPars <- c(p[1],p[2]) -->
<!--   #last tw parameters are the initial populations for x and y, respectively -->
<!--   newState <- c(x = 8.792889e-3, y = 1.595545) -->
<!--   #time-grid is the same as before, no need to redefine -->
<!--   Time_ex5 <- seq(0, 8, by = 0.2) -->
<!--   Time_ex5 -->
<!--   #kinetics -->
<!-- newLotVmod <- function (Time, State, newPars) { -->
<!--     with(as.list(c(State, newPars)), { -->
<!--         dx = x*(30 - p[1]*y) -->
<!--         dy = -y*(p[2] - 6*x) -->
<!--         return(list(c(dx, dy))) -->
<!--     }) -->
<!-- } -->
<!--   new_out1 <- as.data.frame(ode(func = newLotVmod, y = newState, parms = newPars, times = Time_ex5, method = "radau", atol = 1e-6, rtol = 1e-6)) -->
<!--   #generate vector of predictions to align with vector of observations -->
<!--   #predictions1<-data.frame(t_pred1<-times, -->
<!--    #               x_pred1<-new_out1$x, -->
<!--     #              y_pred1<-new_out1$y) -->
<!--  new_out1$x -->
<!-- new_out1$y -->
<!--   sse<-sum((x_obs_ex5 - new_out1$x)^2) + sum((y_obs_ex5 - new_out1$y)^2) -->
<!--   return(sse) -->
<!-- } -->
<!-- #determine_sse_ex5(c(5,2)) -->
<!-- ``` -->
<!-- ```{r rejection-method-for-ex-5, ECHO=TRUE} -->
<!-- # recall the dataset -->
<!-- # define functions to draw values for Km and Vmax from the prior distributions -->
<!-- draw_beta <- function () { -->
<!--   return (runif(1, min=2, max=30)) -->
<!-- } -->
<!-- draw_gamma <- function () { -->
<!--   return (runif(1, min=1, max=4)) -->
<!-- } -->
<!-- # sampling algorithm: returns the sampled Km and Vmax values and whether this pair is accepted -->
<!-- sample_by_rejection <- function (n_iterations, acceptance_threshold) { -->
<!--   accepted <- vector(length = n_iterations) -->
<!--   sampled_beta <- vector(length = n_iterations, mode = "numeric") -->
<!--   sampled_gamma <- vector (length = n_iterations, mode = "numeric") -->
<!--   for (i in 1:n_iterations){ -->
<!--     beta <- draw_beta() -->
<!--     gamma <- draw_gamma() -->
<!--     if (determine_sse_ex5(c(beta,gamma)) < acceptance_threshold) { -->
<!--       accepted[i] <- 1 -->
<!--     } else { -->
<!--       accepted[i] <- 0 -->
<!--       } -->
<!--     #accepted_or_rejected[i] = accept_or_reject_function(true_data, simulated_data, acceptance_threshold) -->
<!--     sampled_beta[i] = beta -->
<!--     sampled_gamma[i] = gamma -->
<!--   } -->
<!--   return(data.frame(cbind("accepted" = accepted, "sampled_betas" = sampled_beta, "sampled_gammas" = sampled_gamma))) -->
<!-- } -->
<!-- ``` -->
<!-- ```{r, echo=FALSE} -->
<!-- # set seed -->
<!-- set.seed(132) -->
<!-- # simulate 200000 times with a threshold of 0.15 -->
<!-- sampled_parameter_values_squared_distances = sample_by_rejection(2000, 400) -->
<!-- # report the number of accepted values among the 200000 samples -->
<!-- sum(sampled_parameter_values_squared_distances$accepted) -->
<!-- ``` -->
<!-- ```{r plot-rejection-histograms, echo=FALSE} -->
<!-- c1 <- rgb(173,216,230,max = 255, alpha = 80, names = "lt.blue") -->
<!-- c2 <- rgb(255,192,203, max = 255, alpha = 80, names = "lt.pink") -->
<!-- post1 <- hist(sampled_parameter_values_squared_distances[which(sampled_parameter_values_squared_distances$accepted==1),3], col=c2, xlab = "beta", xlim=c(1,7), main='') -->
<!-- rect(-1, 0, 1, 35, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- legend("topright",legend=c("Prior Distribution for gamma", "Posterior Distribution for gamma"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- #prior2 = hist(sampled_parameter_values_squared_distances[1:2500,2],col = c1, -->
<!-- #              xlab = "Vm", main="Prior Distribution for Vm") -->
<!-- post2 = hist(sampled_parameter_values_squared_distances[which(sampled_parameter_values_squared_distances$accepted==1),2],col = c2, xlab = "gamma", xlim=c(2,30),main='') -->
<!-- rect(-1, 0, 1, 50, col=c1, border=par("fg"), lwd=par("lwd"), xpd=FALSE) -->
<!-- legend("topright",legend=c("Prior Distribution for beta", "Posterior Distribution for beta"), -->
<!--        inset=.02, fill=c(c1,c2), cex=0.8) -->
<!-- ``` -->
<!-- ### Optimal Experimental Design  -->
<!-- <p> Experimental design involves choices about what experiments to perform, what measurements to take, and, for time-varying processes, when to take those measurements. Most experimental design exercises are carried out in the face of constraints on available resources (time, personnel, reagents, funding, etc.). The goal of experimental design is then to identify the 'best' experiments to execute, given the imposed resource constraints. In cases where we can express the 'quality' experimental results in terms of a numerical performance, and can likewsise express constraints numerically, we can then frame the experimental design task as an optimization task; this is _optimal experimental design_. There is a rich history of optimal experimental design for linear systems (Pukelsheim, 2006.). Here, we consider a more challenging task of optimal experimental design in a nonlinear setting. Consider the case in which a nonlinear regression has been applied to find a best fit model to some pre-existing data, along with an estimate of the quality of that fit. We then consider an experimental design task that targets the refinement of that estimate: the `best' experimental results will be those that most shrink the confidence intervals when that data is included in the regression analysis. </p> -->
<!-- <p> We quantify the degree of confidence in the parameter estimate with the _Fisher Information Matrix_ (FIM) which is constructed from (i) the sensitivities of the model predictions to the values of the model parameters, and (ii) the variance in the data. The sensitivities account for how much effect a changing the value of model parameters affects the model predictions that will be used for calibration. If the parameter values have little influence over those predictions, then we can't expect to have much confidence in estimate the parameter values based on tuning of those predictions. Likewise, if the dataset shows wide variation, then we'll expect to have less confidence from tuning the model to agree with the observed behaviours. A simple measure of overall confidence is provided by the determinant of the Fisher information matrix. (This is referred to as D-optimality in a common alphabetic classification of related optimality objectives). This determinant is a estimate of the size of a 95% confidence ellipsoid that corresponds to a level set of the cost surface (Figure XYZ). </p> -->
<!-- [IMPLEMENT D-optimal design for simple model, e.g. Michaelis-Menton or Hill.] -->
<!-- ### Optimal distribution of intracellular metabolic fluxes (Flux Balance Analysis) -->
<!-- <p> We've been focusing on the use of iterative optimization algorithms to solve nonlinear optimization tasks. For some problems, such sophisticated (and computationally intensive) techniques are not need. One important class of such tasks is referred to as _linear programming_, in which we seek to find the optima of a linear objective function while in as search space that is bounded by linear constraints. An example: -->
<!--  \begin{eqnarray*} -->
<!--  \mbox{maximize} \ 3x_1 - 4x_2 -->
<!--  \mbox{subject to} \ 2x_1 + 5x_2 < 3 \ \mbox{and} \ -6x_1 + 4x_2 < -6 -->
<!--  \end{eqnarray*} </p> -->
<!-- <p> There are iterative algorithms for solving such problems, that are guaranteed to reach the solution after a finite number of steps (in contrast to the global optimization algorithms discussed earlier). These algorithms, such as the simplex method, step from vertex to vertex within the valid solution space until they arrive at the desired extremum. [Fig XYZ] </p> -->
<!-- [Implement solution to toy problem.] -->
<!-- <p> Linear programming task arise regularly in planning, scheduling, and other management tasks. One biological context in which they occur is prediction of  the distribution of intracellular metabolic fluxes. Genomic analysis allows us to identify the suite of metabolic enzyme that can be expressed within a given cell, and thus identify the set of metabolic processes that may be active in that cell. If we think of these metabolic processes occurring in the context of balanced cell growth, we can safely assume that the net production rate of all metabolic intermediates is zero (i.e.~rate of production is balanced by rate of consumption). If we assign a label to the rate of each reaction (i.e.~the reaction flux) in an intracellular metabolic network, this balance condition imposes a set of linear constraints on the components of a flux vector. For example, consider an idealized cell with a simple metabolism as in figiure XYZ. The balance conditions at intermediate metabolites $s_1$, $s_2$, $s_3$, and $s_4$ gives rise to constraints: -->
<!--  $v_1=v_3$, $v_2=v_4$, $v_3+v_4=v_5$, $v_5=v_6+v_7$. Next, consider the case in which the rates of uptake and secretion of (at least some) nutrients and products have been measured. In our case, suppose we have measured $v_1=1 uM/hr$, $v_2=3 um/hr$. .... Not surprisingly, these few measurements of fluxes outside the cell do not provide enough information to determine the values of all fluxes in the intracellular metabolic network. (This is described by saying that the system is _underdetermined_.) Nevertheless, there are cases in which we want to make such a prediction. To address such cases, the technique of Flux Balance Analysis imposes an assumption about the cell's internal flux allocation. The assumption is that the cell regulates the internal fluxes to maximize some objective. In the case of microbial cells, this is most commonly presumed to be growth rate. With this objective in place, expressed typically in terms of maximizing biomass: written as a linear combination of particular intracellular reactions, the task of predicting all intracellular fluxes becomes a linear programming task. </p> -->
<!--  [implement example] -->
</div>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">8.7</span> References</h2>
<p>Ashyraliyev, M., Fomekong‐Nanfack, Y., Kaandorp, J. A., &amp; Blom, J. G. (2009). Systems biology: parameter estimation for biochemical models. The FEBS journal, 276(4), 886-902.</p>
<p><!-- Lenhart, Suzanne, and John T. Workman. Optimal control applied to biological models. CRC press, 2007. --></p>
<p><!-- Lin, Qun, Ryan Loxton, and Kok Lay Teo. "The control parameterization method for nonlinear optimal control: a survey." Journal of Industrial and management optimization 10.1 (2014): 275-309. --></p>
<p>Beaumont, Mark A. “Approximate bayesian computation.” Annual review of statistics and its application 6 (2019): 379-403.</p>
<p>Cho, Yong-Soon, and Hyeong-Seok Lim. “Comparison of various estimation methods for the parameters of Michaelis-Menten equation based on in vitro elimination kinetic simulation data.” Translational and clinical pharmacology 26.1 (2018): 39-47.</p>
<p>Fairway, Julien, (2002) Practical Regression and Anova using R. <a href="https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf" class="uri">https://cran.r-project.org/doc/contrib/Faraway-PRA.pdf</a></p>
<p>Ritz, C and Streibig, J. C., Nonlinear Regression with R, Springer, 2008.</p>
<p><!-- Pukelsheim, Friedrich. Optimal design of experiments. Society for Industrial and Applied Mathematics, 2006. --></p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="machine-learning.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
